{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "from keras.models import Model,load_model\n",
    "from keras import Input\n",
    "from keras.layers import concatenate,Dense,Dropout\n",
    "from keras.preprocessing.image import  ImageDataGenerator\n",
    "import keras.callbacks as kcallbacks\n",
    "import os\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "import operator\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17306 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#generate images from train set and validation set\n",
    "TARGET_SIZE=(224,224)\n",
    "INPUT_SIZE=(224,224,3)\n",
    "BATCHSIZE=128\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        './test_224/',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate labels indicating disease (1) or normal (0)\n",
    "label=validation_generator.class_indices\n",
    "label={v: k for k, v in label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '0', 1: '1', 2: '2', 3: '3', 4: '4'}\n"
     ]
    }
   ],
   "source": [
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ./test_224/0\\100043.png\n"
     ]
    }
   ],
   "source": [
    "#read images from validation folder\n",
    "rootdir = './test_224/'\n",
    "test_laels = []\n",
    "test_images=[]\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if not (file.endswith(\".jpeg\"))|(file.endswith(\".jpg\"))|(file.endswith(\".png\")):\n",
    "            continue\n",
    "        test_laels.append(subdir.split('/')[-1])\n",
    "        test_images.append(os.path.join(subdir, file))\n",
    "        \n",
    "print(test_laels[0],test_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 5 trained CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " #load model 1: xception\n",
    "xception_model=load_model('./xception.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #load model 2: VGG16\n",
    "vgg_model=load_model('./VGG16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #load model 3: VGG19\n",
    "vgg19_model=load_model('./VGG19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #load model 4: inception\n",
    "incep_model=load_model('./inception.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #load model 5: inceptionresnet\n",
    "inres_model=load_model('./inceptionresnet.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the original CNN base models to make predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted result for the first image: 0\n",
      "Confidence level: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQEAAAD8CAYAAAB3lxGOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABZ30lEQVR4nO29f6x2XVkmdq3n/Hg/QZQC9pPyo4DiJNqkqAQn0VpbOuOPtP1q01D4Q0GJPxJJa2JTP3DSGs0kzFQ0NG1IMZhC44Am6EgM7ciYmplJiiMwjKIMCgxEvuD3qThi5vN7z3nOWf3jee7zXs91rnuttZ/nOe95znmfO9nZa6+99t7r132t677X2nuXWiv2spe9PLgyu+4M7GUve7le2YPAXvbygMseBPaylwdc9iCwl7084LIHgb3s5QGXPQjsZS8PuFwZCJRSvqOU8olSyidLKY9e1XP2spe9bCblKtYJlFIOAPwhgL8F4HMAfgfAa2qtf7D1h+1lL3vZSK6KCbwCwCdrrZ+utZ4AeA+AR67oWXvZy142kMMruu/zAPwxHX8OwDdlie/cuVOf/vSnT3pArRXBYiLcOx49d7+klHKx3yScxWnZXNzUMO9ns9nFcyOs+9a5VppSCmqtOD8/X9nHlh230um5Xltvep7bxW29871tqjzxxBN/Vmv9Co2/KhDoSinlBwH8IAA87WlPw7d/+7cPX3t+fo6zszPM5/OLjY97Yd1r3NnZ2VUV+0IODg5WtsPDwzSudS6LA7BSttiyY43P0kfcbDbDnTt30u2hhx7CnTt3cHx8vHKs57NrSyk4PT3F3bt3cffuXZycnFzsY+Pjp556ysbHMd/n5OQEp6enF6AAjAGmi2udPzw8xOHhIY6Oji6FeR9b67wLz2bTiPxb3/rWz7r4qzIHHgPwAjp+/jLuQmqtb6+1vrzW+vI7d+5MfsA2Ru2b+t5ENgpsuzyb3O8qWBWXe+q9p46e28j7pvdYZ7RfR64KBH4HwEtLKS8upRwDeDWA923zAetSopskUzrR/ayPkedcRX64PkbvfdVA38qHnptaH/drkLoSc6DWOi+lvAHAPwJwAOAXaq2/fxXPAlYrd0pF7zqITBnxr2iWJ31+r+7ut39l2xI+iW3KyP2uo86uzCdQa30/gPdf1f0bz52c5rrBYFvP3yWl2zWmtgt1s0v1wfJArhhk7znvr0umjppZZxql6VPyta7sChPQGZORtFOkVUY9twv14eSBBIGQXUJml5fr7jSbgs1NM82u038wJc225VaAwHUry1XJ1FmA+2VzboO1bHLPKel3wTG4K4wzk1sBAuvKrjZKTzYZoafILkyT3SbZBbbj5FaAwLqVuyuNso2po6soyyb33BXl35U2BnanTlRuBQhkMtoBdqmjrCtZB7uKOth1W/82tOf9lGtbNswy1Yk0m81Qa8VsNsNsNsP5+flF+ODg4OIcp+HjbB/XskJNQe9trBDrrTGPsuixbq6MUU8RjusjvO669MzmzdbrR7zbOG9nZ2fp9dpOrTX2/F5CLKs+Ozu7WAvQWgocm64bcNPMEadpsxmKrK5H3zHYluwMCBwfHw+nr7VerF+Php3P5xd7jYt19XF8dnbW3HPHi+fxs7OwO9eaJtKOytvBwcFKp40y6PsEGhfhqKOzszPMZrOLepnP52lH4jwzgDBIaPpMud37BlEOB1qu05+enl5s/A4Dr/nnuuS6Ozw8bKY5PDy8eEckA6seAOl518YqPbByAK91FvW4LSC4sSDACh8dPfYKCgoIPTBwIOBGh9Zxdo7LHHvtCJnS8zG/lOLOA1ipoxEAiI1ZEbOFrC2c8keYX/ZyAKCKz/Uzn88vgYCyBs5j5Pvs7AyHh6tdWxUr8snK3Nq7txRLKZfeUuy1c9bmrQFBBwOux23IjQUBVfho2BYAxPlgBQ4EYoRojQBTt+goGR1ssQFVcN7c22kKAq0R140kCgbOXHDpVRkcC2BA6uUjzIEAAH4jUkGa6zJGec5fnGeWMJ/Pm+ZIb4v2DCCIOopnOSBwbd5T/BZD3IOAGf21w6mCu7hQen1l1o0IOuplVFjjQpEcPe0BgAOC1munkSaUWEcarUfNazAhd23LfMhMATZDWvRf7xsgoG0TYQcCUW8qWp+cP2YsGu4dc/4jz5kp0DIDpvQDjrtVIDCbzdb2CTAYOCbgRnsNu+MWFczORUPyuQCAESDgjqz2beyzd8/1vXQGgalKnAFB5Ffvoder0rQYQHa/aFMFFR2NuQ7DjxH3iThu48PDQ+uzcM/hfhHHAU4uz/zclrQUP2MFGSBsQ3YCBNb1CejI74CAR49NQKC3caOGbRgdhim1lnsdJuAU/+joCMfHxxfnAVzqXK4eszL2Ru2W6cMKxTa480kEaGZAFPcK9qcAwPUYShH3jnJE2/I91F/B7a6sMPLv8q51kZk2rZFf+0ALCG4tE1jXHFCTIFNqVXA2A6ITsJ3oRh1HBx1V5AZmAOCyamfJGr5lDrDiazhAIOu0UYfZKM7K26LtfK+sruI+CgA9QGFwyJiYMwe43K1rVdndIKImjeZ7Pp+v5NGVrQX8DgyYCXKf0EFhDwJLEFCHoLPtHQhoGvUPKBDwPpSagSeOoyycT3Yg9ZhA1uhuBsCxAAYEpspad85pycob9aEjFW9qSjgmsAmbUGrvtjindRnmhEvLW/QF3aL+47gFXrF3YJ+1tR73mICahzsDAqWUFwB4F4CHAVQAb6+1vrWU8pMAfgDAny6Tvqkuvi3QuheOjo6Gnx2VrqN/NuorAGRpnK2oNJFt/JaNGHtHEzWsncChfjY7wADAQMDgw/nKRn92mikjyUCsByhxr9b1fB81R1rtr+LYTit9+JV4Oz09XZlGzsBPt1HG1GIBzixwys/HrWdNkU2YwBzAj9VaP1JKeQaAD5dSPrA893O11p8ZvdFstvho5agwiveUWCm+mgE95ednMOiMerkdVdTjlg3Y8wWE4sdHPRUEXH4yX0CAKHe4Hr2N+2bmwAiAMHhEe6gz0gGn5qlFxx1IsPIHAMS+xV64zOo8zRifHrfAYMQfsBNMoNb6eQCfX4b/qpTycSw+NT5Z1jUH2ATIFNmN+pGWAUGvd85E/jKvUjuXR6fsruzaEdQM4M35A5xfwOVHFY/LrZ0rYwOte0a7cHniuHddXBveeweyXEeOXrv0GsfgoqP/6elp+lzNu7KWFgC5dnZ5cnnNTIFgAttgA1vxCZRSXgTg6wH8NoBvBvCGUsr3AvgQFmzhLzrXTzYHeo6/1uju0mh6nlZkW7FlI0be2Fxp2YutUUBNAffp6YwRsE+gZbOrJ18XorjO6doi9gF6ygKcueTyw6DMlFfrBsCKE5Dr0oFXBmq8ilTLPMLunO+D88SSMQQHBD1TgOO3IRuDQCnlSwG8F8CP1lq/WEp5G4CfxsJP8NMA3gLg+811F/8deM5znjOZCWTzuqO2fu/6UH6u9MxO5HwxTQzl1/TqLGx11t4MAfsC2CRo2dtK2dlX4jqbA4KWw03BQNvOgUBMCbJfwi2O4Y4fYac8eo3uFQQyH0hWj8FYOL8tRdfjdUwC5yvahmwEAqWUIywA4Bdrrb8CALXWx+n8zwP4dXdtrfXtAN4OAF/1VV9Vp4AAgO6o7qb7RkHh/Px8xTbsjRAtB1lvZHGNv4ljMICAp9HcDIAqf0yRjpoCWnYGOy6bqx8GcS6jHscW+edOHwrKgMp+FDe7oqM+0/9sJG+BqfOdtOj5iNJng0LGDLYhm8wOFADvAPDxWuvPUvxzl/4CAPhuAB/r3Ws2m75iUE0AVeLoPAwGum8xAkeLeyDANuKIEvWobDZF6MwA3XoOwKOjowsKntHizFZ1ZednRZnOzs5WlNfVk4KBllvfBow86AwC29At4Iy6DLDQssa9+JlK/7WOXD/Rds7YgNZtCwgcq9mGbMIEvhnA9wD4vVLKR5dxbwLwmlLKy7AwBz4D4Id6NyplumOQFdopOoOBA4DePnMUcR40P7q561Txs1GhxwR6swQZCDDg8f0cEGQAkI3w7PfgumE/CftIZrPVNR462kV7sOnBIODAgdlA63df2r5OoVr1F/4LraeWuPZ27e9Axm3XDgK11n8GwJV68r8G1gEB7Ty9kX7qfoQBRF7YRswcRiPmQIsNKBi0nIMBApnyxwpJHhlbTKBHdaeAgGNKWldR7kzRSyk4ODi4BAx8rZpLGo40vbZtmVGunhQMFDizQSADhIwF7AQIbFPWBYGWIo8CQgsERgDAOYsyEHHlHmECLRag/oDYnAnQAoMMCBxj0fKz/4PBwJ1XMyMLsxnglNwBhJoSupKS64oda60yab0pa5pq9mXt3qL+WdxOOAa3JaVMmyIE7jkGo0M4pc6AYAQMpip/3J87Is8mZHSxRQcdC3AMwPkG1I5lxR9R/qxzaxmYqrM/gOk/2/+q8DoKMjsIp1/GEJxjUEGT64dBUpkAl8WZd7FGxNVbD+y53hyoOgBQUHP94YFmAgCGRnQNq1KMgIAKe9sdEIRp0lIkLnc2Ik6dHeDpwTt37lywJQWA09PTprMs69g9cyAkFD2AIMIc7+ivbjEoaJ1we2U+gRYAxC/R+W1LLUdG/3X2wgFm5IXzpHnsAUKPFdxKEJjN1psd0IbSuIwSOzDQ867TZzZi7EcoYtYpXIdy9DZjAdrZefRiBnB8fLyyXNYBQYsN8Kjp2qXX0UfigAXIRzyP8Jmz0I2aDAYMAA899NCFX4CnADP26HwobmahB5YsI4qf9QVmMduQnQCBdcyBljK3FD9TYI1TaXnbQ+GyESLr8Bx2tFdZQMsx6MwBZwbM53McHR1dAEDmdW7RXA6zEvHoz+nW2bMPIezwyLtjAlp3XE9cLw899NAlEGiZAPqCkZoDzOAyAGiN+KNA0GIem8rOgMBUc4CV0PkFpgKCggKLcxbpto6HfR0gUHNAlZ+nCFXxj4+PrTnQAoCWvcuK7461nKPHAFb8K1xeZgJ8Pddf5hwMlvQlX/IlFyDArEIHAwWAAM+pDlQta0vxe4yAn33rmIDaZz2JxgqHk4adwvN5t4VDKvbZ5kZPbpjRDhFlb3UKZwuOgEOt1Sp75hAcdQ62RjwgNxPWEf3ijzoEtR5bwOm+u1BKWQF+Bc3YGABa9efqytVjL27URLhVIBCj1hRx9n1m94+wAnUWnpycpJv7EEVsGcC0yu4W3Tjw4S/u6muwJycnODw8vFgAU2u1X+qNzSmRjqYMHKFA4VcYoaKjtrGTg4ODCxuenZ4tBuOYinPwnZ6eopTS/Ipx1E/LzIg64XZmP8Xh4eGlWQn9+AsvXmopObOV2B5oEMicOKNgoGF3fHJygrt3714oPv8Ig7+F7xRMO4VuXI6WTerAwNmpPFLdvXv3onO0ftzhptgcC+FOHwAQbAnI35hbNy7iFQTi7UhVHGZgrDhcPld/pSx+bpK1GYtjGAEA2WwFgAsQcC948afgMj9D1ufDX3GrfAIxak0R9eA6UGhNH7p0jglkQBAK6D5n1gKCKG8GBhkAOE9/0NNgAjx6R526PGadNqPUAQBskx8crL7M48KbnDs8PFzx6LPyZDMaPcVhJgCgCZCRJwZG558IVpUtXHKLlXQVowMANrmyftFimFNkZ0BgKhPoKXNP6XvnM1MgYwLOflUQiLxzOVosQBs+RnzHBKJznpycXJgDLn+9Ec91+Oj0XE+qdOrhz8Kt83wco6iCACtOpjyubnmRVPQ1125OmbVOtC60DAwC2bSubs4kyMoSz37gzQGnzC0g4OPsNWMFgRhhGQBin5kCOtJmpkCUI/MJZEzATVmpxz9ooubV5S3EjXrKBiJPsU4jrhvZZ+ey9AcHBxfK78wBpzgKJK4u2WeSAWRWL1wfZ2dnFw7YjAUECET96d6ZNS2nnwLAA20OMAhkTrQsfhQ8eNRXAGj5BNTn0AKBKEuLEfR8AgEGbuqoZQ5onpxfQBfocJl4JGops54bBQKm0u5Dqi0WwHXr7OgwlxhcHThynTAAjILFbHb5G5EKBG7GJgM0dhqXcu//FpvKToAAgI2YgDamU3gd/XugoArPx6x8mWOwR72jHC1/gALBCAAoCGRAlYGAG/EiPZejBQIaHk3HYabSCgCZOcDKk822BBCwj6DHBBQUOY2rNzfCZ1O0vXJwWZQJPNDmQDZC9kZ+BoMMFJy97RyCLZ+AA4AeG2BFy1gA59EpfwYCU7zgag5kI9/5+eWfbWzzeDabNe3nluJwHmPAmM1WPw/H7CADR8eMzs/PV1a3KlM4ODi48D1kazt6azQyVhP9oxT/Add1ZWdAYF1zQL3mLWDoHSsQ6CxACwQcALByjzKB3gxBiwFoJwr24ka8EVOAQYAllKilxK34LK3mg6mzs6tbdnTGrjIQYPaV2fiZ/a/1FTMpfO2UPZdHlZ/LtzMgUEr5DIC/AnAGYF5rfXkp5VkAfgnAi7D4utCrauOLw+uCQA8ARgCiFa+UX4/Z3nadqTczEPuWPyADghhxGAx0RFQG02ICqoDRoTMHIo+Uev02NjdDoccMBAoyWqfKNNkkiK3lLA0QcOc4r9yfNE1v47RaHgArYLxTILCU/6TW+md0/CiA36y1vrmU8ujy+Mezi9eZHciUPNsrMGQA4bzwI9uIKeDKEfueYzAUPzrawcG9H2XoZ7KicyhQ6WjX8myfn583O74yAafEm4CAMpzeUt3W7MDZ2eV3QWKGwLE3LnOk0xmReObZWf5lq169sOJnabh/AFjJ466BgMojAL5tGX4ngN9CBwSmMoGWcmfhESAYudY9owUELTBQ5VdHljNVWAkcAMTm8ttjAjyyqfBzmAm0OvhUoOA4VnRVfAUAZw44kyXOBZC5jc2kKDfHnZ+fr9RTdh+uV76fq4ssLec59pGHXQKBCuA3SikVwP9RF58Sf7je++Lwn2Dxv8IVKfLfgXWZQEv5e8fZuVFfQ2ZSZCOumgNucwCSgUBGKQFcOMJcvnuzAw4EWEGDIWTK3FPu1iio53Rzo38LCAIM9DhAwIGwMoFgRcyCmCmpLydjfpnSunhXDo7bFgAA2wGBb6m1PlZK+XcBfKCU8q/4ZK21LgECEn/x34GXvOQldR0mkCnzyLnWyN5yGrY2vsaxgMi7lqVlCrD93xoFlUIGVVUQyfIV1zDtDVFwiGt7DKBHfUfTZucytsH17AAgrs+AmMvN9RJ7ZgvZ9Xre9eHMR6TpWum3IRuDQK31seX+iVLKrwJ4BYDHy/L/A6WU5wJ4onOPtZiAer71eB27PsKOkjuFcmkyiuk6SOx1VOJ7KQuYz/s/CQnKqE5GZwpE+lAOjYuRk6lvKMWokq6r2ABWRvgRKs39hAFLzYMRpXKmRIBkpty6V3tewxkY6f0V0LYFBJv+gejpAGZ18UPSpwP42wB+CsD7ALwWwJuX+19r3WddnwBvbt4+29z0np5zyuOcP619yxzgsrSYgGMbLeWntrG+Bg7rqNcCAc1jT7mn7LNznA8Xbh1nCt67bytu6rUtc8Mdu8EgOx+AsA3ZlAk8DOBXl4U/BPAPaq3/TynldwD8cinl9QA+C+BVrZtMZQLn5+dDANCa3utdm43o2WifOQMzGzFD/lEAyEbEuHeL7iooBWDoyNq6D4CuIrfie+HWSJ1RZg5ne07n6tCxEC5r1M2I4zPrQ62+xdcpCGv6bclGIFBr/TSA/9DE/zmAV064z2QmkC3f5WPdTwGHTKGzY5f+7OwsVSAuS+teCgCxdx0RWFVefUZ2HNfFtW5hjLtmVLGzY3a2aTp9rqvDjB47mu3u1QIrrpMRwHL3UXbomCKDvJap1U+ib21DdmLFIDDt3YFgAk7Je6v7WmkVBFo0LqNqjrb1TAE1G7ihYw46AECVH/B2KyvSiDhK20uvnX8dpc/Su/oZpdR8bUalo47C18F1pnXL6TiPfOzO9RzIPNoriLs2VeC4VSCwLhMYUfhWmtYyYGfPO4XVuN55LoOWKWMCLecfH/MzdUTTfWbT9uxgpsmZAve2kfRRnpazNegz1zGbEao4CiYMALwQiM2kyKuuU4h3A3TP6Vjp2f8U5wLU1Y8RLIL7BveluPbWgcBUJpAptYZ7xxkwRL5a20iaEVPAAYAqPwOAgoG7F79HAGDlGmf7tmzb7Nopis+jJStWCwScw5XDpZQV5Z7NZit1rPWqMziansvFcZx/ffOvtT87O7Msk1mda0PXvtHv90yA0qtiO2WfEq/heI4qe4R130uncVyWUTBwisv3if35+fnFyJTZtJn96wCnFddT+JF4FwfAUujYc12w78XVq7uPmkzhgGNgcExAvwOgLzgxSJyd3ft3YfQt98KTM1vceWU2tw4E1mECvY0rn7+yMwIIkS/ec37d8Wg6Ps5sVh7tevZ/dq/WKBudne3PdRXbnesd9+IArIyevD4iRlGug6DQLTua78cgwGVXr7vmzX3GnF9x5mNV/oODA5ycnFwa4VXBHRNwjsRtzRDcWBDgRlWk17n/VpzzH2he1kHc1jWZnd0yI6ITRN4cpdc64hWG/DxVFkfxp772qnEtp9kIUwAumzBaH07xuf6d4rDPxzkqeUEUt4nWk5oGDhRUiTlP/J0Gdv5mZVGAv3UgAExTNGdfh2QjFb8Wq9e7UXAkb62RPguHuA9LZi/FZHZ8q25cHfXsfc6HvrjD4d76ffdqby+f3NHjfOYL0DAPBG4WSMH/9PT0QgF7PhKtu9GyAEgZZzYj1Vp52mrXTWVnQGAdUfuW57czBOeOGjQtGoE/Jd2y/7exzxxKqmQtCq4goWVtOfqcAjtQcgCgdq22A+cjJFN0HdnjPsEOR6d2WytG3QrRmJdnZsV5jb1O+6qiumfFPyDCMdjb2FR1y9Azn1JrMJgiOwMCm6Cbo5RuFNTOzQCgI4oCyehxL23Y4MFOWkDQAgA3kk8FA8eAMuru8uUYRTzbtW0oUij5bLbql4hzQfNbyjwSbl3Pawo4n84EY7OC/zcQjj9mGuwIjPjR6ekIt9gAt+22GMHOgMAUUTR0TIDnfdmZNJvd+xAEN2ALBHpb5GlKenU0Ze/L9+h3Cwi4flqK72z43tYCH95zmyn9VuXXc9nKzpaij7KBYB7ap6YAwXw+X/kxS4BAsAEFgVGWwv1QTQGWPRPA5Q7OQJB1frUnHbpnINBbFNRKo7SOmUBmDvRs7549O4UNtMChBQLxPLfntuWRK445rxpmc2BE+bNzWXwMBK4fZsrPfSWUXqcB9Q9FTrmd0ju/QAYEWp+bys6AwDrCnZg7WIzwagLo4pNWWKftOKyeXneulQ7A8Gg7sk1lAZnCZ/6AzCcQz9A2caI2beu6YActRZ7CBjInnOapxwDUBAhz0n33MLz/bo1CtvbBpVPH4EhdT5WdAAFXwF76EO7cGsfzrjEVo42bbU55W+Ep14Y9PAIEygoy+p+xAq0jvX4d0+Dw8HC4A2bt2pveChBYh/rrddl0sTPp3BQeK77a/9nqQQUBBYRsxO/NELDcKhBYV1oAEIo/opAjyp0pvBsxeukVBFre9zjH5s66LCBjAj0wcHlk2h6iI2sW37um1npJ4dh7zp732GcUu2VzO1NO5+9ZMYNNKgNw5lxMSWfss8VK3TkFrFtpDmzCBIB7C0vYXmLv8xS7Ps45NM5Mh6zxXPrIWwYEGR1v+QJavgEOj/gAMsXPQMDZqUqxNW3vmM2BTZmA21zbq/IHIKjyq7JnX0IeHTimsFP1p2xD1gaBUsrfwOLfAiEvAfA/AXgmgB8A8KfL+DfVWt+/7nMG8nGx10pyHYzDrQ45gtQt9NZ9MJMYgUq5/DXd1r7lKFRlj/2IM9CBQQuUGAQceLLo6N4CXd0chdfRnKfhRpWf6XaUoQX0I2aaYwIMAqNsNGOomT/g2plArfUTAF62zNABgMcA/CqA7wPwc7XWn5l4v8l5yJxLjm6641Y4o23OfnO2XIRD+WNmgtlKpuAtAMhs+2zU12MFgpZjsOUXYBDgTpvVZ8bIXIeP+mt50Z1ytwBAzQtuC/UVOeV3oOji+JXiFgsdBcUMCLbFAoDtmQOvBPCpWutn183cVBBw9u9I2pHzMRK1nDbuHHe2oJAMBmqyuNG9dzziHNSyZYo/AgQtc4AVP9ovOi2DRJx3I58bIbU+MybgGMAIg1AQYKbGgD2Vnel5rpcRM2j03Gi/HpVtgcCrAbybjt9QSvleAB8C8GO18QuyTcRRYN1PTRNh7TC9bT6fX3QeVXxVvPPz8yEFzEAgc+5N3VomQZansIGjrCFsiilVzWzvlt/FKXSPDfTSsRkRbRBKz4ofdc5tmLVDr41GmOc6x9yHN5Vt/IvwGMB/CeCNy6i3AfhpAHW5fwuA7zfXXfx85BnPeMYmz5/sKBsZSR19VFrJ4eg8ChoBDnxvBYFMCVsdr6f4rm6mKH1mAwcIaAcMZQJWFwOFZADQ8qVMNQd6I786Bh2QMgNo1fsIaLq+mvXhVv/O+nzv2lHZBhP4TgAfqbU+DgCxB4BSys8D+HV3UaWfj3zlV35lnWIOOFNAO3gvThVJlUoVv9W51GvMaZyCRvxIR3LpMhDr1dEIGLhZCef8cp0v2lA7qDMFRkytXt2rY7AHHAreWb9Q8y1rj164xzq1bVzdZXsNbyLbAIHXgEyBsvzpyPLwuwF8bAvPWBGmnctnDjXaFCVrUcn5/N6LIqr8cQ9W9GyUHlH0rEw9NpNtvXs7JuK839wBnUMrA4nMC58xrREmkKVzYX5BpzdoOIBoDSqbtklriz5DerYbIFAWPxz5WwB+iKL/finlZViYA5+Rc1bcFEjnuSvhrDNnTjZH93QePutw8YIIK79SZt7m89WPgHD+teOEcvU6mIJBjxG0Oh/fK3NyZY7BaLvZbPUvxa5t1Rxwyp7tp9L91vW87ymjq+N14qaARwuQXP/fhmz634F/C+DZEvc9G+Vo7LkA/MdAZrPVL7+6juzOKRAw5eSOFy+J6GejdAovU9LIN3AZBFqdpRen9x9R/qz+egDAswNuPlzBwM2DOxMgU/JRAOjFaTzXVSgZ1+lIXY6M8KMMlDeeQQpx+dqG3MgVg66BuANnW7ayy4EEKzyDgdr//PHIFgBwOTn/LYUePZc9a7STlrLKAhQQXT1GWYIJjLKBzAxQ0B2x9UdHfAcgDAJcV67eNA6492ei7Lz2yXUY6sHB6mfhrmqGYGdAYF1pgYC+0KFxWRoGgdhi9FcQCCBQlFcU1zxr3qco7Og5foYDzN7m2FNswL0R3pU9AwK30jJjAiOj/jqmBIOAtseUfS8NK7b+l4DDvE4h4nQ9QAAss+AHngloQ2gF6+udU7bZbLbybri+LcYgkFG9liJoOba1ad1kit8ChIwZaRxw72Om0YldubMFQxkTcCbYVMUfYQhhmnBbaB9r9b+RY+2PLUaq38CMOs760q0EgSmiaKhUXEd3/hJsdszfkI/R/eTkJAWInuJnjaRgN6LQo+fcfV3azAfQYwMOBJgJtPIzMjPgPsjJI7cqc6bsvalB9gn02mcTcYxT+yUDgAMmXs8w1YE+KjsDAttYJ8CddvTb8Bx3fHy8AgKHh4cXQOBQ3dF/pYWt5aAjoziHp6bjfGSAkjmpWiyAQYDn1Ln83K49n0CLDbSUvgUMI9dy/lp90fVNt/pP00U/dD8lCXOIX10+OjpauX42W33fxA0g25AbCQK6RFU7ceYUZAA4Pj624WgwdYypwgN+tM+Wy7J3XOfS9Z7Zfuq53tYzDVqsAFisqjw4OLBz7lmb6WIhBwbZB0F66wnc+VacgrLmleOzdK39wcHBxWfGQvm5H2hf1gElAFYHkG2aAsCOgMBUmtN6+cTRTqZabtO0WXoWVTZWlGg8B0R8Xzdi673dfiRNy4Rw57NO5Z7dEldfWb0DsODo2i9rN+dz0HBLYpSdIj3lzwBl1KzLzDUdmG4dE5jPx/5AxBU1n1/+qSNTJ9exehSUzQAeleLb8M5u5VHN2XXRaEH/AP+tvZHwSLpspFcZUdQMWIHLYOyUuQWkWR04/0J4zyNc6+WPxoRXPcL8HD7WfuLqwu2zesuuc/4m9Uupf8rts3PZ7NNU2QkQiA4zKkyVlKLH/WI/CgTxmejw/Pd+cJGBgHZ2ZQlRTk3D0kJ4d85dn6VzddXbnLIDfUbWYgUu7w4MeO1BKL1zlqnyuziVlnLrcS+ti2OfgCp+5qyekubWMYHMW+vELUppOaRU+Vvfjde/Eo3MVzMLYDBzFC/rkCwxSrn9yLX67Ck2ZE/5W2ZXRt35vuvkW0HAKb9u7k3HkbJvklbj3BRzixFoOGMG2Zuc68pOgECtdRIIZB1cO102HRWOmlgNGAuBdC1Az7mkx6oEnFcGgHXs0KkyBQRaZkBL2XnRzwijaOU1y7OO+HGc5VvLldnOCqx67TYUrDdN7ZS9FafHt84cmAICPHXC99BOHPSbldbN0epxOPd0lM882uoPcOaAxo2MJL347Hym/BlbcvWnYKDTgdFmPbYwhQ24/MezHCCo3d+qr/DFxP3Vd5PVURY3kiZbjJYBwcj5W8sEgHFzoKVAGQhEBw7lDjDQ6UP25PN10WGyTs9pMsdgZgevY2u6+B4Q8Gib3acFAAoGQO4T6DEBBwYtM6BlDkS45y3n8usAovUy1RGb1atT4pg2dP6CONcCCAaDWwUC0XlGpWX/x714EUYAAC/5VaXXLXN0uQ7vFIDzyR7vcHZlCtxS7OzY0doppgDXYYRbdr86Bp0JMWoWKEA508ApPQNrjwXE/TgceXWKnO1H42Lv2KZTdpdmZLtVIABMcwxmZoB21NaKN30NWBcI6b3X2SKvase6JaDaoXvA4BRf2YZ21IyBZEzAgWtvijAzBXrtqIDJzt+MCXCdZmXK6H2mzE6hW3GtNMoyHRhkbCFTfD7XA/hRGQKBUsovAPjPATxRa/0PlnHPwuK/Ay/C4uMhr6q1/kVZ5OqtAL4LwJMAXldr/Ujr/kG1RyVMAlUoN/K7xRZuRWALBDSvvM/ScT7ZgaMjbhafgYLbs5Mr4uOZrY7SA4JtzQ6MAELklcOjip85yBytZ3PAsQ6+LjOnekARYbdsWPc9/1QWF311GzLKBP5PAP8bgHdR3KMAfrPW+uZSyqPL4x/H4puDL11u34TFh0e/qXXzTUAgFoiwDyAch+7FGLf6yi0P1ue5PLTyx+GWHR7hDACmhDUPbrGQYwStUd8pu/MJ8HW8d/lt1dmIT0CZVVavrXsHm2kp+ibnAFgm4EBg3fB9NQdqrf+klPIiiX4EwLctw+8E8FtYgMAjAN5VF63xwVLKM8vqdwetrAsC0SkYAGIL+19fcFGl5zgGgZEOwPERdnEadiNv79jFubqIfTbCOWmBAS/SCZoO5CsGHSOIZ/Ce27NV30754z5uNMzK3AKYqZvez/UZ9T31QCA7l4HBtmSTOz1Miv0nAB5ehp8H4I8p3eeWcSkIrMsEWOk5zEo9n89tfC+sDazxLswdkjuKWwYb5c6Unc/FqJWlcUCgI34PAPj+TvFD+RUEHGtYxyTQ/GVTg07xen0l8q0vOvF510auD0xJo++NODBw8a1zHH9fmUBPaq21lDJp9Uuh/w489NBDk2YHgHvfuY991hCqwD1F1rQjG49ITFM1Hww0kWZkUyUfUSwHOk7cKJ2BQSmr02v8Nl7LJ8D3b4nm1wHBlHu4jV/NbSnw1OMMBHQWyk1HZ0ygdX5XQODxsqT5pZTnAnhiGf8YgBdQuucv41ak0n8HvuzLvqxOYQLAvWmeUZRupYv7qcK2HIu6504ei1gcAMQ1owAwOu3G6ePZWi4Gn2yvAMAsgEfVLI8ZGPBzeu2qQDDaHyJv2f00/1nfyAaEVpyLb01Ft4Bg5HhXpgjfB+C1AN683P8axb+hlPIeLByCf1k7/gBgmk8AgO0omVLrsYvXNK01BNEIta6uWotrubNzB+EZCABNJeeOOgIK0fkdbXZ1ppLdM4BAR9Nos2x2oAUECgajbaIKp/l398ymGnUAUSV3DLLFFjMQGOlHI/Eu3X0FgVLKu7FwAj6nlPI5AP8zFsr/y6WU1wP4LIBXLZO/H4vpwU9iMUX4fb371zrNJ7DM08U+rnWd3u01zqXrOWy0o/O9QhEjjllA3MfRbVYgVn59Tkv5M6+3K7veKwMCVZy4fmSNQGaycP1ou2YgwIofdefup8rP4QwA1gm3gEJBfx0g6G33FQRqra9JTr3SpK0AfmRqRqb6BDJxFRMKlZ1zcdlSzWwhDHcC7ZDaKQIEWHGjc+roz4DAzjkHIgDsyN0a/fW4BQBcJmB8dkDvn7Vbiwmo8sbzmVXxXpWf69Up7sjxaFrX3iNg4NK3AOK+gsBVyzpMIK5b93zvWv2Yw8hLQrE2gSU6oXaCyINSbu6sqvScb97zPYB7P6nIRn+thxYD4DArFtB/d6AFBlpHXFcKAlG2nvJz/nSvIMD37il9KNwoIOi1oyDQOu/OPfAg4DpVtp96rpR7vyELADg+PrbKr3S/xRCY/sXz2PHmwIA7c6scCgScPzfCZvWZgYHeh/M+Mj3I93binsF7BQJXDwoIyqp6IOAUuHW+t6kCZ0ru0h4e5v8pYGDahuwECADTHYOtDubCU9KVUnB8fHzxvQDt5DpiRaOwQgB+doAXeqgHXkfcuIcLq/Lr/XqmQFaXDgDUF8Ag4BhSBgijws9ShuOUn9kS582xqlpXlyBnit8CgB448PlM+d25LH0GBtuSnQCBaKQp6a9yA+7NgTsTwAFA76MiYRJwA/LIlG0OGKIOQjLnXeYX4LAyIgcEek1I63sCrbplaZU9JJRW/R4cH2Ed9bPjeLZTWlVyp9QjxyOKnYHFSNy2ZCdAAJjGBNxo05uempK21nrpa0HxXACXGj37vFiINjgzAVVaZQZxb1cHKqMOwaw+lQH0wGDdWYEeM8jyzoruRnhuK82PA4FMcZ3CtcBgqlKPXNPb3zoQCKWbkp4bPbNNs7jeHrjciUJcYxweHl5SBMAzhvD28vnMJIiNl7u24ka3Xr32mBkrmtZxCwy4Hl0+1gGwMMNU0VXpdVN2loHACCj0lHUKo5gCFNuSnQCBUIRRYQrIVBi4NxrGfdWOjb3ez3XU3mjFz3B+gpa9yde2FHqKaBkyZXYjf+Qxi+ct8qWfXnN7Bwyubrneas0/Juryz22agZCrnwCBluJquWOvbaR7russ7PKmzEbvrY7fbcjOgMCdO3eG07tRfiQ8GgcAd+7cwUMPPYQ7d+7gzp07OD4+Xtn0N2bu3W8dSbgTRbl5z+VzmypcKxz31Q4cXmWOa4Wzc4BfJzD63UFtfwaBVh1k5yI+A4+MjbRAjvta1j5xTQAmf4fRDQJTnIpZfIS3JTsDAsfHx8Ppe3R+U7MAwIXy6+bAgEHBgYBrSKak2ajfKp+OujoCR70q+EQndYo9Zc/5G20LxwhU+SMuyjACAhrHx637ZOWKc9wOCgbKlrK66lH+KZtety3ZCRCYzWbDTMCNjqNg0ItXEDg+Pr4EALFvMQG2/XUEYB+AU/7WaJeNvO4ryMDl15g17M6NxmkeVcEdLW8xgegHcV+m+KMK7475ngwI8VxnxikAAPfMzLhvAFV2fYtl9Eb9bGM2d+tAYCoTAPy77KMjfQYA3FFY4XXvTAIFA3YaZo2p1JRlivK7Le7p7NgMGEbSO8qu+Z2isDr6sqL17rHuOa13Nst6Nn3kz12jrK4HAlOV38VvQ3YGBKb6BLLRpzfKj8QBuFB0VnxlAc4MyN4H1wbUUUTLp/sMFLIfpMQo2BrtWttIGs1nFs7Oc/urcvUUW+tl6jVOeuYAl1nbzF3rFLh3PCXtrQKB2Wy2NhPIRnMHDCNpFQRamzIB5xNw1C8bfVzHazECNQFiqXOc7yk5P3/q1pPRNJnicR24cO/8SJifk0kP4HrlU+UdCY+mvVUgMJUJAEgVeRvHAC5Genb+aTi21uehMyYQ5XYN6fLmHIMKAByOkctRVR6p4lwGChqn+d5kr3nk81wXXCetuN75qXGjecj2qrAtZe6duyoAAG4oCLgRct24Fggw5XczAe730ZlJoCwgU6pWWZ3yOybAIKD3XyecnR9lDMo+uN057JjGpko8kl4dlppudONrgom1lHwEEFr32JbsBAjMZuOzA0DuExhV+t55AJbqOxMg4gIMsjUCygbcKOjK6Eb/jAm0QGBk30ujcVln5g6rI5eCASs+d3Kuh2w/ZVTO4kJZdXZImQD3F+dP4nZyIOCUuVVv2V7D25AuCBT/45H/BcB/AeAEwKcAfF+t9d+UxWfJPw7gE8vLP1hr/eGBZ0z2CUxV7B4gKAjwCD+6jZoDrvG1bLrP2ID7e7KuFeB6Hg2PxI14rwPwQrHjWO+b0d1Nlb+1ZwCIY3bauv6jH1Z1G4NACxRb50bS3DcQgP/xyAcAvLHWOi+l/D0Ab8TinwMA8Kla68umZKKU6T6BKTRtBAD4nsC9n0kq5XcmwDog4Chzr3w9c0Cdg1rHrt57cVmaTPG1vFynLLp0OjZeTxH1sI29i4u6Ywnl5WsyRpat1ozjTOl7Sj6aZlvSBYFqfjxSa/0NOvwggP9mk0ysaw5kSrzOxtcDl0FgJDxiDvDo6Bo0y5OOMm6KkM0B7txTO4wqoBNV+NhHvtx7ErFXYFBQiaXNnIcpyj2SPup/pMzaBhkA6zlV6t42AgTXAgID8v1Y/JMw5MWllH8B4IsA/k6t9Z+6iwr9d+DLv/zLNzYHXNy6aYD83/Luu4MHB5d/KZ05BjMmMFLGFgAoEKzzubZ45khcjNoKesy63JtuzqGVsQp+9hQFz+L0vLIlt3YjAwA3K9NasLUOCER9ta7ZhmwEAqWUnwAwB/CLy6jPA3hhrfXPSynfCOAfllK+rtb6Rb220n8Hnv/859dNzAE9HlH+7JqQ6ODu908j8VMXC7kG7VHRrEMqCPQUOwu34gIE4ruKUWY30xLpo9zqGOTzzATc6DxFyXtxYd9H/QZDU+dgBgQMuHo8n88vyrXu1rt+W7I2CJRSXoeFw/CVdVlbtda7AO4uwx8upXwKwNcA+FDrXlPNgeX9Jyn5lGMAK0rslgLzcfbX2dbsgHrMea95UzNAv3rkZgcCBLY1amrcbLb4OMrZ2dnK9xTic+wqDAAOINQkiPrRe42A10iYFV3rWp/XAmFW+vl8jtPTUwsCWs6ekmfpA0CvHQRKKd8B4H8E8B/XWp+k+K8A8IVa61kp5SVY/Jn40wP3m+wTiL0qMcdnaaaCAG9ulG8dKwCMILpjKOydbjECBoEMAEbqrHcuQEA/w+5kNrv3iq3WuVN+9p+sAwIj6fj+AU5Zu3AbZD6Y09PTlX2AAJcxnstxqvBZODu/DRmZInQ/HnkjgDsAPrDMTEwFfiuAnyqlnAI4B/DDtdYvDDxjsk8AmAYCvbDGOQBwgKBvCro02sGjzCP0boQRtN4f2BYw6vFsNrsAgGyOXRWc02r7Rzp947Kl3FOPXZgB1U29OSbgAIBBILYom+6nhlvntyEjswPuxyPvSNK+F8B7p2ZiXXOgt++Nbq29juKO3rvjXtpNmEDLL6DmQIxE7l6ujjQ+o8s8krqp1xA3ujMIaFr1CRwcrP7fUdu9F+fiFQQUUFsswIGvA4LT01OcnJw0QSDbr3NuG7ITKwanmgPAdKVuMQCXTgFAaX12rpXGLYgZAYNsarBlDjAITFkoNbrxTICO7k6pw4GYAYADjVHlHjmn8Zz3MFWcn4br0NW7A4AAgVr7qzZH465C+UN2AgS2yQRa56ZckznzVMF7ce4ckFO6DLRGgEAdg6r8DgwygOhdq9OBXCYtdwBAy3cQCsgMqtf2654H7oFANn3L93L1nvkDGAgYBKJ+WuFeXCt+E9kJEFjXJwCMe71H0nCYR2y3rXsunGMtFpDR9ZZJ4MwBp8C9cHZej6Mc3IY6qh8cHGA+n1tToMcc3BoD1049ydKGOZOt48jaITPDWPkZBFic4mbA0Du+lSAwlQkAY97hFhC07sGU3Sm3nm+lGzl2ZctG58wh6JYOZ0q+6V6VNFPkYALqPNTrHINoMaVRaZkHodDsiOz5BDLTK0yAk5MT3L17dxgENj23DdkJEFjHHGAZAYOR4xYIuJHOdeBemlIu/3Uonqf5cgDQmqvWjqmee77ehfX+ykACgHogwDMjOqWpTEDrpgcCrbacIlGWjAUwG2jVO5sDDAB3794dzt9VK3lPdgIEAL+cdFS2NTqwqNKzwrJd7wBixN5X5erFOWbAyjRyTpVR34hzJkcWV2vF6enphbLP5/OLfZgAylCyac3sWZlyrDNqajxPCbaYXmsgcFvmV3D9wB330mZxm8jOgMC6a91DpnSMHvLqCO0UW0cKly4LA1hZVKIjd6Z0mS3tqDRP8TkJJQhlc+UdEVZcHfUZGAIwmCmcnp5aBlWK/wmq1n3Lxs72rXs5R66CnC6Ocm0SezVJHTBreOTctmUnQKDWurLCalRGG7h3zej9OL8cZoo/ei9eYKJr/lv0me+V0eiYXot7jQorXtzTvQjTYiXZaM9AEMqfjaa11otFVk6Z1wlnNN8BuJozZ2dnODo6SpmWA2L3HsU2NtcHN5UbCwKuYbXhVUFdnLvOPcvl2YWz9CoMAhlFdrTe5dc547SDjozwTiG43rJrHBtg5Y88BRC4KTmlz7p4Z50tK3/Wxq4us/9MZtcxe3C+FbfPwm7PdbQtufEg0Gv4bIR29+HjyJdeM2qjteLCnnYv/bTMAZdvtx6BlyazMmV5izoKX4eCQSZKWd30JYOAm0p1da5MgBnPusdcPidZXfZAwNU9v1GZtefIuVJW/8QE3FvfsE3ZCRAApvsEpo4IU67nNG7Ez/YjaaIjORBgVtDqeJp/1xGdneqk1tpU/l5djpgDAQQOADh/fJ+jo6MVZe5NzepeHX9cnqy8cY6VmZXStYNjYaenpzg6OrIveznfibZ5xHG+HJhvS3YCBNZlAi30V9R3DECPlUFw/jisCs3hkeNwmqlPIPOWOxuU88ydMKa9gDawOnCazVb/CNwDUi2TdmoFgR7o8v14Ka+CRy8uAIABLp7DZdO2d87V83P/enTGwMJ5GK8Uq3mkYZ1JYQCIvbbTtoHgxoIAN7ZD+jgXcc4HEGEHJD1HjHPajGxKlXvmgAKAy3c2M8BpHSC0FL4VzuohA4LZbIbT09NLozJfD6yu5w9zIFuK3VqiHWXiTQeFSMN15OpR6z6r85g9iJmEYAKtLw8FYEe6cMQqA8j6wLbkRoMAK3tQ4GhsHRHiOQwIbkR1I59jAtzxXZiPNU0oScs5ODo9yCMhd1xXx3rMtDi2uFdWR64enC8gOvR8Pm+aFNw2fD2/ps0jbYTjGe6FLe4PDjyzulQFd0uXI42+Mh5AfnR0dAnYda9xAQBcV1w/XJbo18pWN5EbCQJRAarkGdozC8iooGMCoVCuwp2Su33mDAoQ2AQAXKftOY50dFTF5/rqmQPung4MmK25e2XX9l7TjvLGOVaYuG/UV+wzBVLTytUh13Xkj0GgtXpTQSGuUb+F1lG0Jyv/fTcHiv/vwE8C+AEAf7pM9qZa6/uX594I4PUAzgD8d7XWfzSSkamOwWiMTt4vKVILALgxtMK547BSZqNhxDkb340EU2cHOP9KTVUJQrQueIRxij8CBD0AaF3vGFNcG59sy5Rfp/C4LDyCu/ZugSuDCdchM4BQyFBmXk8Qi76iPdUBHGsl1JyJ7w9kddOry01k3f8OAMDP1Vp/hiNKKV8L4NUAvg7AvwfgH5dSvqbW2tTwdcwB7ewtJclooaZjVGalyEbWbJon8/jqOTd6tGYHMnOAfSMOBFQ5HBuIDtdS/CkmATu41A8Q1zrmpPZyKFkLAMJx5xRb887tq/WYXRsjP+8DeBjonUM0nIQBBjF7wCCgfZHrNuqyxaQ2lbX+O9CQRwC8py4+OPqvSymfBPAKAP9f5xmXkLAloZgOrTVNdGxOp/fKGEFck3V8bShWXvX2tjqLAkHGBjjPnPds9MqUnzt8hKOjOQbU6niuLtS5NXKNgiN/yzEAgT/kGtc4IGi1sbOpHRCwU5nrht+K7A0CofxRFmYBJycnzfpx9+wx33VlE5/AG0op34vFl4R/rNb6FwCeh8XPSEI+t4y7JIX+O/CsZz1rmAn0RmdO45iAG015Y/sy0vA+pAcEOqo7xc/i41xrdHMdN1P6CAdLcI4m3asSZ6MVPyeuYyDga3oAygAQx/pF4xh9dVmu1g3XETOSKGMGFqz8fA3nNepPzTwNx5uFJycnl0yAjCFFfTpG1bpmE1kXBN4G4KcB1OX+LVj8hGRYKv134IUvfGGd4hMIZc3oaktBOK1el00ztQDAIXbYha3RXlmDMgnHAloA5jq2Az+2MWPvFN+ZBcqI+BlxPd9L29TVGzv3dMosptkUFJgFjABlABMDgKtL4PIbopxfF26d55HfgUAGAGwKOefqtmUtEKi1Ph7hUsrPA/j15eFjAF5ASZ+/jOvK1NmBLF4bPkN9Tg9c/tMLd2aVjALrCK/OvxYAKPIrCPTMAVc3CgLMAhQEMjDQemrVQ3TgTNxoqfY/z7mHgzCU3z2v1weiTFqXCu7MGh171GdmeeHjMAUyAGBGpX0p9q3rtiXr/nfgubXWzy8PvxvAx5bh9wH4B6WUn8XCMfhSAP+8d79ap08RaphpX8YCep1GWYAbDTnPjglk88Aa1o6p93Ajjst3y05UEyHuqQrfAkDHttz9Ob2CJ5sekZaV3k0HKhA4djTKkBQM9DpntnDeW+FWnL4+7fqTA1EGRjfFet9BoPj/DnxbKeVlWJgDnwHwQ8sC/X4p5ZcB/AEWvyf7kdqZGVheNwkEXIW40V8poCnbyrXaYVo0OMJKAxUIsrnizJ7ke4509qgPjuNRjTdeS6Cjv1LUllmgbadAoOeYfbAJ4Do6A8Gmyq/efK0Pzus6TreeMp6cnAwpMfcBNn14TcG1mgN1wn8Hlun/LoC/OyUT64AAsNrw6vxx9l9rRFUg4LDLr3Z+Hcmd8vPiIKfoes+MxbDyuxGYj919Vencpo69rN3inq36CQBQ0GElZQCezWYrtn9G5x2wR73EQhw1BxRIFPh03zrX2+uPZzLld7NK+valA+is/FNlJ1YMAtMWC0WncyxgSodpKX8PuVumgHMI8mepnfef7+uOHXgBqy/ERMdwTCAUj1+M0Q7aYlgZIIwAATMNp/DKwkKJ3XoJB4jabrwOX8ucMYGM+fTqpJWeQSDrQ8og9R0EZxJsW3YCBKYyAUffY8/0b2Q01fu5jqnpI88ODBwAMBNgEIj7uH0WF3lRZed0HB/5UjOAj11dOjOgBQSxjzrn52cMqxXWF3h6YKhlCCBgn4MOEHqfbCBo1VErHSuwCjNB11/0F3fX6hO4H7KOOeAaosUCMurUQnVX6e6+rjGdScArx+JevXrJREFM68f5Gxwg9ABAnxF7lzc1Rxio9FxvayltD8BjIGAHW9Yn3Eje2jiNm/fnjZ2B2q5cNvYV6YyCe4Zr803kxoJAq9FaZgFLRucUZLI8O5s1cwzqX2sdHVVpKWGWjuN0BHTzzj0G0Bt5nDJx3a4bVsWNZ2kZWwqcrcDM+oIqsNvzeY7TcGwMiI7+BwhEXmOZsQOBXp9cV3YCBIDpPoEMeXs+gVEgaCmB3o8b1E0R6m+qFAScAnE851XDrb1+GSczC9ipmpU9qwc97uU3y7+GGQTc/Z3ys8JOWYWp91IF5G8buMU/2ctNUZc905EHivAH6KvUmVmxDdkJEFjXHGAPMI9yCgaOGrfMBXXMcbzeSxvVAYJbLcijJ+DBKNJw4ztFcNdzPY3SW1UqDmfP6LWrC4f0GIYqGHvNuR61/kdnETgf2WDAis5goJRdbfiY3ZjP5zg+Pr5YARmbU/gMSLj9by0TcN7lnnDDsx0Y4JCNrvxMRmce0Q8ODuyvpmOvr4dmK/ycHdsa/bMROKuzVn2OmkAOGNSbfn6+mL/mewcb0/KptDps6/qDgwMcHx/jzp07OD4+vgjH8dHR0cX+6Ojo4rNerFROwTJ7vsX4YmO2qiN81EmYo2dnZ7bfuP7T+voQ98+MzW4qOwECwLSCuVHYfZWF7+1GDLXboyMFCCiF14bUbwRmNqcqHH+zLjMDeO/KE53QxXMdcSfies5GvREfSnT0Teh+63w8w4FAhEeOFSRaoMDP5v7CdRvl5r3zF8R29+5d/PVf//XF9tRTT+Gpp57C3bt3Lzb+lXnrm5MuT9uSnQGBKYVj518wgGwEzWyxGN1ihVaAAL/zzfP63ECOCbSAwClZSKY87hxLBgARP0KJHRuI1X0OyNhEYBBo7dc9FysGVamnbEy/MyDQfhP1qP2R2U8wz8x0iuMAgVB+BwAKBO4bEz0H96ayMyAwpWBuZHeUrueQ4UUZ/M53NLJb95/FZd5nN9qOlHUUAHj0V1+GTq9leXIAwHlgACilXHTMzHxphbM4ly+m/azYrbCaCU751enmBg4Al0ZgZ7JpnvlYQcAxAmUB7ruTbqp0m0CwEyAwFd1UoVv2XGv6znWMcP5kK/9ax9pQITx6hrJxPnt1k6XNOoT6OjhNpnCz2eVv9IWo05BnN0Y2vUcGDLwFE8iUO1N4Pu/AQD9b5sCW64zZ1Qjz4XLdvXt3hQWE4jMjUHOgxQb2PgGRULgYmfReag+z0y/2vD5bX/lkwMg8/u6cAwJWoDh2wKeNzKN8HI9co3UwAgQKTNlIF3XUAwE31TjlfAC0KnYoc6b82cZsgJmAk21R7pOTkxXFz/wBag44dsnbtmVnQGBK4ZiWqjiPbcyPRydm5df5YAYBVexWXGYORH5jpI3jKG826rgyaZzuNdwzCSJPboRhhsUmQ9Rl5FeVme3jTPFHrglgVkU+Pj5eiedZgVZa5xRU+u9s7lZcVv9xzCCgm/MHZJ+g3/sEGumd88b5C3gE07lft9KLQUAVKYtzjjgeybmTq8Jr4+o5rSN3XeYPcKAUz1CGovHsCQ8GFXUK4JLybnPPU3ysxJny8yjv4pURBPMJNtlSfte2WXszuIeiO+XvzQ7oLMFVAsFOgMA6hXLMgRtCV8bxOoJsSidb5TV1a5kDDA6te/TqaSQPWafJAEDPKcOJ/DMIqGdcmUN2rhfOQGB0y4CB2UCUN+pJ28QxPAX+1nGsEdCRn5X/7t27zWlnnSK8Cln3vwO/BOBvLJM8E8C/qbW+rCy+SvxxAJ9YnvtgrfWHRzIypYCaNiqIO1/Y/a5j9jouU/eM+umzs/PRsdw93aiSgZsrb2tkcgDAHd0BQSiEntPOH1R6tC5bdd/anB0fyqyLgRwQtOLZAaoAAFx2PvM+gFBNw1jcE3EBAqNbBgA9VreprPXfgVrrfxvhUspbAPwlpf9UrfVlUzMy1SfAtIs7KncyXkOgHu7MFlWPMYfjeS6+l1+9RhkD51/9BZmowus+YxdqovAoz6CV3Sfu5ZiUKnjGunqbOmuzmZyR89kMkAN6rdfMEZwd89ZadcqK31qJej/YwEb/HSiL3vQqAP/pphmZygR0hGUK2/NCA5enrLJtWc5L+03igHu2qCo8AwFf4zppRv97IBDCdeHum8VHfTsld0rfS5MdO0V36/V1G0nHppkyMK1TnRLOpoo5Xl8fd0uHs621/kRNzW3Ipj6B/wjA47XWP6K4F5dS/gWALwL4O7XWf9q7yToI56gtMLZApZduBDCmgojbouNxGFh9Lz+rF2UQTFOdGcB7rYd1zJ1SVj+aoU5XF5elya5RpVWlbqXTKV+XLphiAF7USQYAI5um1YVAuvzcHWezA1cBAMDmIPAaAO+m488DeGGt9c9LKd8I4B+WUr6u1vpFvbDQz0ee9rSnTTYHAD/v7dK5873wFGeWHrt76rUODNRWVzDhMjvbP6ON2nEYPF29uWv0HIOAUzan0L3zLdCYcq/ePXl6MJSL66HFBJxCt/Zuc/8nzJaj7/QUYSnlEMB/DeAbI64ufj92dxn+cCnlUwC+Bou/FK1IpZ+PPPvZz65TzYGrlqkUNpQfuPfdP2AVANhmVgVX+p0xGC6/OhR7zqMREBw9diCQjby9457CjpoOI+ZIhAMIuS1Y1C/AynlyctKk+pGmt/S8xSSytwmvQjZhAv8ZgH9Va/1cRJRSvgLAF2qtZ6WUl2Dx34FPj9xsHXNAr+uFW6OjHvc6btjufI6FaTYrNHdCNwJHXnikbqVx/gCe8lLan7Gjlv/CnWspck/ZMxvdxfW2XjplbNxWwZ7YFIh4ZQHMBFrOPt6rUme+g14annm4Cjaw1n8Haq3vwOLvw++W5N8K4KdKKacAzgH8cK31C71nTC2U2qnuWM/1zsdxyGw2Sx1MvK+1XozwvDlQ4U6oJg3nZTabrVDUbJTSTdlAxiZ6vooRv4YqbevDGHq+lVZf7mkpc89UcyabmgLKylx9hqdfR3/dYs4/9uzcy2YQWhu/mXpVAACs/98B1FpfZ+LeC+C962RkKtVpTV9lCjIlvZtvPju79y+8AABnb7NpoB1SbVKqOwsEvXTODIgw5yPy4vKlyj1yHGXJlDubxsvCeuyU2e2zcCsuzDVeTKb17JgAAwErfrYcuPV+iZtizM7vumNwa7IOE3Adf3Tfuh7ApZVmzkmjEorrwCGjo06pW/4AVw9u5AphUFJ20FKSXhwzJafQLRBwi3n0PD+vt42m47RRXy1AzhyDzAb0rUDesndMdIGRS6Ppe31vE7mxIJDZwVyxbqQcOa61Xqw1n8/vfRtOgYKFR0elbaw4QXW5LNzpmAGoWaDXcFg7TkiMetrRWSkyut2ysZ1St5R7JK2CAeczM01cmhFzh+tbr+M6bc37s8Lra8JPPfXUikPP9bepffIqTAFgR0BgauGyjt9C1hHE5YqPRj8+PraUO7ORVQmByzMEwSxqvfyjUAWCXh0oi8me78wWBYKe00297K21+73zmi7e8+f4yOemexcX5hJvrl4dEKgfIDb9ipD2mSnMNEsTedym7AQIAOMFcwDQs63cvnWu1nrxhVi3bFNBIDqRNjqQKxuXJeKcL4A7sdZTZk4we2h1Hkf33fRcNnXnXtQZfaFnBBw4nyojca3jAGBlASHav5QJqDnAXw+KvfaZqH8Xr8yudW7bsjMg4Ch2Jg6lR9Zyu/PuOLPDGACc8jgQAC4zgejgOhKF8jtqq+XPlF9BQIGA4xWcRqb3eMte5XXK79JkH/xQEMj6wDrCCqVmjpoDtdZL/adlDsT25JNPXoCAPlNBfJP9tmQnQGBTc6A3xeIWX3Ca8/PVv8CwPRhKzeIoclB8Z7+p84qZQNB/1yE3AYLIpxtRWJShjEzxZQDgwq10CgIcr34T3vfiRtJqvff6l67sU5NAvyP45JNPbqy0VzHqO9kJEJjNZjg6OhpOH6MxkC8ZDqRXiu2UKlOmUcrmxD0n81Y7W3zkGVpuLV942Z1Sj4yCURelrH7FSVmFq0sOu7pUW1g3VUz37FZcL32Wl157Zu3mTCVmt3rf3vGU8mwqOwECpRQcHx8Pp2cqN5/PL/Y9JY+wOt9aI+6mMuq57gFBdt8o98HBwUVdBJV2jj0HAK6uHJVlBVWb2W1M72M05VE/RlVmA/FnHgaBXru0zmfnmOo7E46vzxSd/SJRjnAmOxap/dDtW+di8LuV5kApBXfu3BlOr/YcA0Dcz13D1zEd7HniXX43AY1oSO5gnDdeR8DXxJ7LHNecna1+Rg249/ZjdF4FG1cOBgBXb/ycHgioV52VPQDBORi5DC3GNxKX7R0DcWacbs5EYlDjdwCcVz/CDLJZmgiz4l8FEOwMCExlAmpnKwhklM8p/6YKraP76DXMAiJf7BDLQICVOUb/s7N7X1I+PDxs1lFWbzrqc4eLuoprnRPVKT8rPR/rFCH/m09XVWYsaiScMa9gApnpp+2V+U3U0dlyKLvNgU9mhrp+sQ3ZCRCYzWZrgYB2ak0Te0b9GMHcSLgpGHC4tbk06sTL7uXMhuwrNFOVKITB0R0zcAXoOMV3I/7IQiI2WXp1ue4G4NLakRYAOCDIAEBBYMQn0krn+vU2ZSdAoJRp5gCPSi27ViuTbdoWJb5qcaN6yynFabNRya01d8/M8hLibFR3bYBAPJv3qvg8uisguLjwCagCOiaThXvHAGydZQCs9e2mO1vr/Z2iq4+FWZfzYzGw782BWlPlj30GAGHTZh3lqiVjDAEELm0pZQX42HQIxeMpTdehW44njh9xWEWeggUwEEU4gGA+n9spRj1W5XIgoOCt5xxoZOcAzwS0HridMnMgACCWlrtN+6Ees5kS7c3+Az6/bbmxIOAU1nVgbYzorG56bBviaCfHRzj27AV3IBAdgkeMyD+bNwp0PVvU2aEhmY3Mx6WUS4qvWwBAb81BtjnlH9my6yLPrFyZU1DbYCoAuL7X2s7Ozi7aOp6rDtqrAoKdAIHZbLaWORDCFIlH/5irZcXhUSuzFUdFr3PXZ/d1z27ZwKH0DAatzuYoqNu7EbB1TYRDody0WSs+S+vS8J6V0Ck4x7fSMIvMloRHfWQmmJaBAYDvMfoeS/RF7pNM/5kpXgVbHfmoyAuw+Nz4wwAqgLfXWt9aSnkWgF8C8CIAnwHwqlrrX5RFLt8K4LsAPAngdbXWj3SeMQwC3Ak13pkAAQS8RLjlExitZE039TrtYBHHIwEr/1SlbnU+HWUcELTobCwccoqdhbNzLaBQhdNwBhbZvQIAgmVk9nurndQvoE7AqEMAK31O+x9fF3nJ+hGDwZS+NiojTGAO4MdqrR8ppTwDwIdLKR8A8DoAv1lrfXMp5VEAjwL4cQDficVnxV4K4JsAvG25T2WqOcAg0LL/2QRQMyAbdXsylTFoZ9I4tvG5A0bDR8d1NF5HZ60D7njxjOiQXH9alw44tCMDsMqcKWdLwVvpRlhE7zo2m5gN8DLxFhPgvuJMm4xJuHdT4rmuLRgMuK8zC7gWJlBr/TwWXxFGrfWvSikfB/A8AI9g8dkxAHgngN/CAgQeAfCuuqiJD5ZSnllKee7yPlZms2lThGorZTTW2auZrTgioyCh4cwcCGElD8VX5XdlbW1uzp7Bbj6fWzuY67H1Pkbku0XDs3NT0unMgcZH2Ck/m4MMAAwCDHSZrR1AEAPJCABEHbMZGvUXz462iPp0baIAcBVAMMknUBY/Ifl6AL8N4GFS7D/BwlwAFgDxx3TZ55ZxKQhMMQeAHARaI5eOOIzwkYdW5U4BgNa9MmBg5Vda6UCAj12YFdYtqeY6Y3FMwH0JNxSpZ4e3bPfeeVV0N63ogIKVXxVVy+kW9mi7ZD6BuL9ex9cwgLZ8P06ijoP5XhsTCCmlfCkW3w/80VrrFzkztdZaSpnksiz034HnPOc5k0GgRf91FMuYwCi69ip+5NrMHIg90/KQFhj00p2fn1+89abl5LrjemixCf1OPuD/zZBtPJqqfe2OVfl13QHHxVQl0/7WKB111FsxqPXGyt+6N5eHR3838Lg2VCYYQMD3j/bahgyBQCnlCAsA+MVa668sox8Pml9KeS6AJ5bxjwF4AV3+/GXcilT678BXf/VX16nmgDMDMhDgqaqsg66LtCPOGjURFBAcAGQy2vDZNCjXlZ7jZzgmwB/ViOsdCDhQaKVx51Th3Y9I9fuPvGhJR2mtN07nHIMKBApkyhw0TWz6gpv2N+0fCgLBArK22oaMzA4UAO8A8PFa68/SqfcBeC2ANy/3v0bxbyilvAcLh+BftvwBy2dMYgJBR9X+Z+XnN9cyv8CmFGsqQ1Aw6IWzZ4zEMf0E/DvyLZBwLIC/qc8gosrc24+cy0CAXzSKdubVeqr42UgddZSxAa3bUP4sTaRrsaIMBLR93LXsDN+2jDCBbwbwPQB+r5Ty0WXcm7BQ/l8upbwewGex+DEpALwfi+nBT2IxRfh9vQeUMn12QDu0AwFercav2U5hAZsygxZTcOeyEcKZFa3n6AyAgkCMhFlnVDDQj2wqk1AF0BE0C2dxs9ls5aMjvAIx4hnsda7eKTXnU+sjs+sjP6qcBweXfzajZXEgEKaUq28d2Fz9XgsTqLX+MwDZk19p0lcAPzIlE7PZtMVC2SKM8/Nz+6aaMwdaqLyuZArvlFyVXcOtuF7aqCPAv0jFjtIeW3CmwOnpqV1slYGCy38vfYD38fHxxWgf333kV5PdUt2MAWhdO7+AOkodEGQAwAARg47WcQt01VRzg5XrX5vKTqwYLGW6OeDs/+gYEXbLU5kNcKfjvPTyqumya6aAQWvL7MiWgs3ncwC5YmsHC2kBAX9brwUCWfl65yMNsLDZ+ZPv/PVnpfFuUwagdRZlHfEHxBbK38p79C/ub07xtb5HzLVrYwL3Q0qZZg5oJ1Db1XmQuYE2dQq6dC2Fz9I6BdE8jcbr+bBf1W/CtDrroM65qr/h0g+5ZAqux1m8HisI6Nef3VRwNtevIBB5dQ7llmOQ4/nebLfHngEgE/d8ZmqZSbBt2QkQ2MQcUMrKQJCtLstMgnWkp/yOOcSxjiA9h5lzoLnzCgIZAMTG+cv8AfqBzQABLeNIeOSag4MDHB8f4/T09AIAmAW4hT5OQfm+Wk/OJ+B8CAwAMYUZG9N3NS8UGEMyHwADQLauwPWlTWUnQGBdJuCUnztsxgbWZQKbKHzsMwrplHzqFspfSrE/wjw9PU2paiiFjk4KAgEEDAJa5kzhR84xCIzS/960aVZXwOq7+i0fQtRteOmjnRREeB1BgEeI+me4rnmdg64yZCC6tUyglOk+AV69ph5sVf7WsmGu4KsoV+98BgatlXW9JbhxP62b+CCmY0QhzjZ1swOnp6ddEOjVRQssDw4OVv7sqyO/U9zMw6/gGuXna/Q+cX2t9z7TFmHeBxDENQoIWsbMBxAA0OqnVwEAwI6AwGw27d0BBQD+am109NY76xkL0Ipep9JbtDf2jg1oJ22tp8+OIxxKxKO4q5esc2XmgK4V6NXBaLyTMAd69n82Fch1rXXF/pBsJoGVXeM4vgVEnF7PRXliitM5r3W151WBwU6AAIBJo3EgsDZyNrJmdmcc835UMhrbckxNvS/fO2MELXbTMn+y57lOm9nOmfTo+agwk1EgaC30cTa+20bzmrEYVfa4X2ytwaZljmaDUsi26jdkZ0BgVFFGK6DXUTd9lpvmuap8hLTqaBPWso17XadoW0w9v85ztiVXcc+pcjXG8A7IVdpQcf+RZ2V0ex3pdfAp0qqb+zHKrytT2zSbMbifsuugemtBYJdlFzpFS5l3IX+jMhWUNgGxUYfntp43JR+byI0HgaxCtkHD1x0tt52PkfxsU24yQIzMyKwr67Qj+wu2IVdR/zceBDK5anPgOvKxqdNxU7mu+mzZ9JuaB3u5BSBwPxxyTkadalfNSPg525JdAM+QXrn0fA8kNinbutfuUn06ufEgkMl1MoFRp+Go6Pz36LlNn7cLsunajR5I7OUWg0DIVTX6KAPZJiO5Xx1410euTeQ6yrbrPpZbCwK6smvbwvdtKfo2GckuOAZ3Ta7KJ3C/6qA1mNwvubEg0FOubZsD9/NZ1y3bmBW5Kpn6/NvULlclNxYErtrxN/L80XSb2qVX1ZGzOtwl+jp1ifK2pgi3uYJ114GoXDeyA0Ap5U8B/FsAf3bdedlAnoObnX/g5pfhpucfuNoy/Pu11q/QyJ0AAQAopXyo1vry687HunLT8w/c/DLc9PwD11OGG2sO7GUve9mO7EFgL3t5wGWXQODt152BDeWm5x+4+WW46fkHrqEMO+MT2Mte9nI9sktMYC972cs1yLWDQCnlO0opnyilfLKU8uh152dUSimfKaX8Xinlo6WUDy3jnlVK+UAp5Y+W+3/nuvPJUkr5hVLKE6WUj1GczXNZyP+6bJffLaV8w/Xl/CKvLv8/WUp5bNkOHy2lfBede+My/58opXz79eT6npRSXlBK+X9LKX9QSvn9Usp/v4y/3jYY+RbbVW0ADgB8CsBLABwD+JcAvvY68zQh758B8ByJ+/sAHl2GHwXw9647n5K/bwXwDQA+1sszFv+T/L8BFAB/E8Bv72j+fxLA/2DSfu2yP90B8OJlPzu45vw/F8A3LMPPAPCHy3xeaxtcNxN4BYBP1lo/XWs9AfAeAI9cc542kUcAvHMZfieA/+r6snJZaq3/BMAXJDrL8yMA3lUX8kEAzyyLX9BfmyT5z+QRAO+ptd6ttf5rLH6Q+4ory9yA1Fo/X2v9yDL8VwA+DuB5uOY2uG4QeB6AP6bjzy3jboJUAL9RSvlwKeUHl3EP13u/Yf8TAA9fT9YmSZbnm9Q2b1jS5V8gE2yn819KeRGArwfw27jmNrhuELjJ8i211m8A8J0AfqSU8q18si743I2aermJeQbwNgBfBeBlAD4P4C3XmpsBKaV8KYD3AvjRWusX+dx1tMF1g8BjAF5Ax89fxu281FofW+6fAPCrWFDNx4OuLfdPXF8OhyXL841om1rr47XWs1rrOYCfxz3Kv5P5L6UcYQEAv1hr/ZVl9LW2wXWDwO8AeGkp5cWllGMArwbwvmvOU1dKKU8vpTwjwgD+NoCPYZH31y6TvRbAr11PDidJluf3AfjepYf6bwL4S6KsOyNiI383Fu0ALPL/6lLKnVLKiwG8FMA/v9/5YymL1wnfAeDjtdafpVPX2wbX6S0lD+gfYuG9/Ynrzs9gnl+Chef5XwL4/cg3gGcD+E0AfwTgHwN41nXnVfL9biwo8ykW9uXrszxj4ZH+35ft8nsAXr6j+f+/lvn73aXSPJfS/8Qy/58A8J07kP9vwYLq/y6Ajy6377ruNtivGNzLXh5wuW5zYC972cs1yx4E9rKXB1z2ILCXvTzgsgeBvezlAZc9COxlLw+47EFgL3t5wGUPAnvZywMuexDYy14ecPn/ASYi8P+5QLRCAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Single image prediction\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "test=cv2.imread(test_images[0])\n",
    "img_show=test[:,:,[2,2,2]]\n",
    "test=test/255.\n",
    "test_shape=(1,)+test.shape\n",
    "test=test.reshape(test_shape)\n",
    "\n",
    "res=xception_model.predict(test)\n",
    "\n",
    "prob=res[0,np.argmax(res,axis=1)[0]]\n",
    "res=label[np.argmax(res,axis=1)[0]]\n",
    "print('Predicted result for the first image: %s'%res)\n",
    "print('Confidence level: %s'%prob)\n",
    "plt.imshow(img_show)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range(length):\n",
    "    inputimg=test_images[i]\n",
    "    test_batch=[]\n",
    "    thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "    #print(thisimg)\n",
    "    test_shape=(1,)+thisimg.shape\n",
    "    thisimg=thisimg.reshape(test_shape)\n",
    "    xception_model_batch=xception_model.predict(thisimg) #use master model to process the input image\n",
    "    #generate result by model 1\n",
    "    prob=xception_model_batch[0,np.argmax(xception_model_batch,axis=1)[0]]\n",
    "    res=label[np.argmax(xception_model_batch,axis=1)[0]]\n",
    "    predict.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xception accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "pre=precision_score(test_laels,predict,average='weighted')\n",
    "re=recall_score(test_laels,predict,average='weighted')\n",
    "f1=f1_score(test_laels,predict,average='weighted')\n",
    "print('Xception accuracy: %s'%acc)\n",
    "print('precision: %s'%pre)\n",
    "print('recall: %s'%re)\n",
    "print('f1: %s'%f1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 50 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range(length):\n",
    "    inputimg=test_images[i]\n",
    "    test_batch=[]\n",
    "    thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "    #print(thisimg)\n",
    "    test_shape=(1,)+thisimg.shape\n",
    "    thisimg=thisimg.reshape(test_shape)\n",
    "    vgg_model_batch=vgg_model.predict(thisimg) #use master model to process the input image\n",
    "    #generate result by model 1\n",
    "    prob=vgg_model_batch[0,np.argmax(vgg_model_batch,axis=1)[0]]\n",
    "    res=label[np.argmax(vgg_model_batch,axis=1)[0]]\n",
    "    predict.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16 accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "pre=precision_score(test_laels,predict,average='weighted')\n",
    "re=recall_score(test_laels,predict,average='weighted')\n",
    "f1=f1_score(test_laels,predict,average='weighted')\n",
    "print('VGG16 accuracy: %s'%acc)\n",
    "print('precision: %s'%pre)\n",
    "print('recall: %s'%re)\n",
    "print('f1: %s'%f1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 56.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range(length):\n",
    "    inputimg=test_images[i]\n",
    "    test_batch=[]\n",
    "    thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "    #print(thisimg)\n",
    "    test_shape=(1,)+thisimg.shape\n",
    "    thisimg=thisimg.reshape(test_shape)\n",
    "    vgg19_model_batch=vgg19_model.predict(thisimg) #use master model to process the input image\n",
    "    #generate result by model 1\n",
    "    prob=vgg19_model_batch[0,np.argmax(vgg19_model_batch,axis=1)[0]]\n",
    "    res=label[np.argmax(vgg19_model_batch,axis=1)[0]]\n",
    "    predict.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG19 accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "pre=precision_score(test_laels,predict,average='weighted')\n",
    "re=recall_score(test_laels,predict,average='weighted')\n",
    "f1=f1_score(test_laels,predict,average='weighted')\n",
    "print('VGG19 accuracy: %s'%acc)\n",
    "print('precision: %s'%pre)\n",
    "print('recall: %s'%re)\n",
    "print('f1: %s'%f1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Inception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range(length):\n",
    "    inputimg=test_images[i]\n",
    "    test_batch=[]\n",
    "    thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "    #print(thisimg)\n",
    "    test_shape=(1,)+thisimg.shape\n",
    "    thisimg=thisimg.reshape(test_shape)\n",
    "    incep_model_batch=incep_model.predict(thisimg) #use master model to process the input image\n",
    "    #generate result by model 1\n",
    "    prob=incep_model_batch[0,np.argmax(incep_model_batch,axis=1)[0]]\n",
    "    res=label[np.argmax(incep_model_batch,axis=1)[0]]\n",
    "    predict.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inception accuracy: 1.0\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "f1: 1.0\n",
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "pre=precision_score(test_laels,predict,average='weighted')\n",
    "re=recall_score(test_laels,predict,average='weighted')\n",
    "f1=f1_score(test_laels,predict,average='weighted')\n",
    "print('inception accuracy: %s'%acc)\n",
    "print('precision: %s'%pre)\n",
    "print('recall: %s'%re)\n",
    "print('f1: %s'%f1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. InceptionResnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range(length):\n",
    "    inputimg=test_images[i]\n",
    "    test_batch=[]\n",
    "    thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "    #print(thisimg)\n",
    "    test_shape=(1,)+thisimg.shape\n",
    "    thisimg=thisimg.reshape(test_shape)\n",
    "    inres_model_batch=inres_model.predict(thisimg) #use master model to process the input image\n",
    "    #generate result by model 1\n",
    "    prob=inres_model_batch[0,np.argmax(inres_model_batch,axis=1)[0]]\n",
    "    res=label[np.argmax(inres_model_batch,axis=1)[0]]\n",
    "    predict.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inceptionresnet accuracy: 0.9998289136013687\n",
      "precision: 0.999829777674089\n",
      "recall: 0.9998289136013687\n",
      "f1: 0.9998288793066084\n",
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    1  170]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       0.99      1.00      1.00       197\n",
      "           4       1.00      0.99      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "pre=precision_score(test_laels,predict,average='weighted')\n",
    "re=recall_score(test_laels,predict,average='weighted')\n",
    "f1=f1_score(test_laels,predict,average='weighted')\n",
    "print('inceptionresnet accuracy: %s'%acc)\n",
    "print('precision: %s'%pre)\n",
    "print('recall: %s'%re)\n",
    "print('f1: %s'%f1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 6. Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " #load model 6: resnet\n",
    "res_model=load_model('./resnet.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range(length):\n",
    "    inputimg=test_images[i]\n",
    "    test_batch=[]\n",
    "    thisimg=np.array(Image.open(inputimg))/255 #read all the images in validation set\n",
    "    #print(thisimg)\n",
    "    test_shape=(1,)+thisimg.shape\n",
    "    thisimg=thisimg.reshape(test_shape)\n",
    "    res_model_batch=res_model.predict(thisimg) #use master model to process the input image\n",
    "    #generate result by model 1\n",
    "    prob=res_model_batch[0,np.argmax(res_model_batch,axis=1)[0]]\n",
    "    res=label[np.argmax(res_model_batch,axis=1)[0]]\n",
    "    predict.append(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet accuracy: 0.9662959794696322\n",
      "precision: 0.9338569031275825\n",
      "recall: 0.9662959794696322\n",
      "f1: 0.9497662530625438\n",
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [ 197    0    0    0    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       0.00      0.00      0.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           0.97      5845\n",
      "   macro avg       0.79      0.80      0.80      5845\n",
      "weighted avg       0.93      0.97      0.95      5845\n",
      "\n",
      "Wall time: 35.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "pre=precision_score(test_laels,predict,average='weighted')\n",
    "re=recall_score(test_laels,predict,average='weighted')\n",
    "f1=f1_score(test_laels,predict,average='weighted')\n",
    "print('resnet accuracy: %s'%acc)\n",
    "print('precision: %s'%pre)\n",
    "print('recall: %s'%re)\n",
    "print('f1: %s'%f1)\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing time is :99.662228 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range((length//127)+1):\n",
    "    inputimg=test_images[127*i:127*(i+1)]\n",
    "    test_batch=[]\n",
    "    for path in inputimg:\n",
    "        thisimg=np.array(Image.open(path))/255\n",
    "        test_batch.append(thisimg)\n",
    "    #generate result by model 1\n",
    "    xception_model_batch=xception_model.predict(np.array(test_batch))\n",
    "    xception_model_batch=list(np.argmax(xception_model_batch,axis=1))\n",
    "    xception_model_batch=[label[con] for con in xception_model_batch]\n",
    "#     print(xception_model_batch)\n",
    "    #generate result by model 2\n",
    "    vgg_model_batch=vgg_model.predict(np.array(test_batch))\n",
    "    vgg_model_batch=list(np.argmax(vgg_model_batch,axis=1))\n",
    "    vgg_model_batch=[label[con] for con in vgg_model_batch]\n",
    "#     print(vgg_model_batch)\n",
    "    #generate result by model 3\n",
    "    vgg19_model_batch=vgg19_model.predict(np.array(test_batch))\n",
    "    vgg19_model_batch=list(np.argmax(vgg19_model_batch,axis=1))\n",
    "    vgg19_model_batch=[label[con] for con in vgg19_model_batch]\n",
    "#     print(vgg19_model_batch)\n",
    "    #generate result by model 4\n",
    "    incep_model_batch=incep_model.predict(np.array(test_batch))\n",
    "    incep_model_batch=list(np.argmax(incep_model_batch,axis=1))\n",
    "    incep_model_batch=[label[con] for con in incep_model_batch]\n",
    "#     print(incep_model_batch)\n",
    "    #generate result by model 5\n",
    "    inres_model_batch=inres_model.predict(np.array(test_batch))\n",
    "    inres_model_batch=list(np.argmax(inres_model_batch,axis=1))\n",
    "    inres_model_batch=[label[con] for con in inres_model_batch]\n",
    "#     print(inres_model_batch)\n",
    "    #bagging the results generated by singular models\n",
    "    predict_batch=[]\n",
    "    for i,j,k,p,q in zip(xception_model_batch,vgg_model_batch,vgg19_model_batch,incep_model_batch,inres_model_batch):\n",
    "        count=defaultdict(int)\n",
    "        count[i]+=1\n",
    "        count[j]+=1\n",
    "        count[k]+=1\n",
    "        count[p]+=1\n",
    "        count[q]+=1\n",
    "        #rank the predicted results in descending order\n",
    "        predict_one=sorted(count.items(), key=operator.itemgetter(1),reverse=True)[0][0]\n",
    "        predict_batch.append(predict_one)\n",
    "#     print('predict:',predict_batch)\n",
    "    predict.append(predict_batch)\n",
    "t2 = time.time()\n",
    "print('The testing time is :%f seconds' % (t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict=sum(predict,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bagging accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "print('bagging accuracy:%s'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Probability Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model,load_model\n",
    "from keras import Input\n",
    "from keras.layers import concatenate,Dense,Flatten,Dropout,Average\n",
    "from keras.preprocessing.image import  ImageDataGenerator\n",
    "import keras.callbacks as kcallbacks\n",
    "import os\n",
    "import math\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.optimizers import SGD\n",
    "import operator\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing time is :2.661651 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "img=Input(shape=(224,224,3),name='img')\n",
    "feature1=xception_model(img)\n",
    "feature2=vgg_model(img)\n",
    "feature3=incep_model(img)\n",
    "for layer in xception_model.layers:  \n",
    "    layer.trainable = False \n",
    "for layer in vgg_model.layers:  \n",
    "    layer.trainable = False  \n",
    "for layer in incep_model.layers:  \n",
    "    layer.trainable = False  \n",
    "output=Average()([feature1,feature2,feature3]) #add the confidence lists generated by 3 models\n",
    "model=Model(inputs=img,outputs=output)\n",
    "\n",
    "#the optimization function\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "t2 = time.time()\n",
    "print('The testing time is :%f seconds' % (t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ./test_224/0\\100015.png\n"
     ]
    }
   ],
   "source": [
    "#read images from validation folder\n",
    "rootdir = './test_224/'\n",
    "test_laels = []\n",
    "test_images=[]\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if not (file.endswith(\".jpeg\"))|(file.endswith(\".jpg\"))|(file.endswith(\".png\")):\n",
    "            continue\n",
    "        test_laels.append(subdir.split('/')[-1])\n",
    "        test_images.append(os.path.join(subdir, file))\n",
    "        \n",
    "print(test_laels[0],test_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing time is :51.721883 seconds\n"
     ]
    }
   ],
   "source": [
    "#test the averaging model on the validation set\n",
    "import time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range((length//127)+1):\n",
    "    inputimg=test_images[127*i:127*(i+1)]\n",
    "    test_batch=[]\n",
    "    for path in inputimg:\n",
    "        thisimg=np.array(Image.open(path))/255\n",
    "        test_batch.append(thisimg)\n",
    "    #print(i, np.array(test_batch).shape)\n",
    "    model_batch=model.predict(np.array(test_batch))\n",
    "    predict_batch=list(np.argmax(model_batch,axis=1))\n",
    "    predict_batch=[label[con] for con in predict_batch]\n",
    "    predict.append(predict_batch)\n",
    "\n",
    "predict=sum(predict,[])\n",
    "\n",
    "t2 = time.time()\n",
    "print('The testing time is :%f seconds' % (t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability Averaging accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "print('Probability Averaging accuracy:%s'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model,load_model\n",
    "from keras import Input\n",
    "from keras.layers import concatenate,Dense,Flatten,Dropout\n",
    "from keras.preprocessing.image import  ImageDataGenerator\n",
    "import keras.callbacks as kcallbacks\n",
    "import os\n",
    "import math\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, LearningRateScheduler\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 block1_conv1\n",
      "2 block1_conv1_bn\n",
      "3 block1_conv1_act\n",
      "4 block1_conv2\n",
      "5 block1_conv2_bn\n",
      "6 block1_conv2_act\n",
      "7 block2_sepconv1\n",
      "8 block2_sepconv1_bn\n",
      "9 block2_sepconv2_act\n",
      "10 block2_sepconv2\n",
      "11 block2_sepconv2_bn\n",
      "12 conv2d_15\n",
      "13 block2_pool\n",
      "14 batch_normalization_1\n",
      "15 add_1\n",
      "16 block3_sepconv1_act\n",
      "17 block3_sepconv1\n",
      "18 block3_sepconv1_bn\n",
      "19 block3_sepconv2_act\n",
      "20 block3_sepconv2\n",
      "21 block3_sepconv2_bn\n",
      "22 conv2d_16\n",
      "23 block3_pool\n",
      "24 batch_normalization_2\n",
      "25 add_2\n",
      "26 block4_sepconv1_act\n",
      "27 block4_sepconv1\n",
      "28 block4_sepconv1_bn\n",
      "29 block4_sepconv2_act\n",
      "30 block4_sepconv2\n",
      "31 block4_sepconv2_bn\n",
      "32 conv2d_17\n",
      "33 block4_pool\n",
      "34 batch_normalization_3\n",
      "35 add_3\n",
      "36 block5_sepconv1_act\n",
      "37 block5_sepconv1\n",
      "38 block5_sepconv1_bn\n",
      "39 block5_sepconv2_act\n",
      "40 block5_sepconv2\n",
      "41 block5_sepconv2_bn\n",
      "42 block5_sepconv3_act\n",
      "43 block5_sepconv3\n",
      "44 block5_sepconv3_bn\n",
      "45 add_4\n",
      "46 block6_sepconv1_act\n",
      "47 block6_sepconv1\n",
      "48 block6_sepconv1_bn\n",
      "49 block6_sepconv2_act\n",
      "50 block6_sepconv2\n",
      "51 block6_sepconv2_bn\n",
      "52 block6_sepconv3_act\n",
      "53 block6_sepconv3\n",
      "54 block6_sepconv3_bn\n",
      "55 add_5\n",
      "56 block7_sepconv1_act\n",
      "57 block7_sepconv1\n",
      "58 block7_sepconv1_bn\n",
      "59 block7_sepconv2_act\n",
      "60 block7_sepconv2\n",
      "61 block7_sepconv2_bn\n",
      "62 block7_sepconv3_act\n",
      "63 block7_sepconv3\n",
      "64 block7_sepconv3_bn\n",
      "65 add_6\n",
      "66 block8_sepconv1_act\n",
      "67 block8_sepconv1\n",
      "68 block8_sepconv1_bn\n",
      "69 block8_sepconv2_act\n",
      "70 block8_sepconv2\n",
      "71 block8_sepconv2_bn\n",
      "72 block8_sepconv3_act\n",
      "73 block8_sepconv3\n",
      "74 block8_sepconv3_bn\n",
      "75 add_7\n",
      "76 block9_sepconv1_act\n",
      "77 block9_sepconv1\n",
      "78 block9_sepconv1_bn\n",
      "79 block9_sepconv2_act\n",
      "80 block9_sepconv2\n",
      "81 block9_sepconv2_bn\n",
      "82 block9_sepconv3_act\n",
      "83 block9_sepconv3\n",
      "84 block9_sepconv3_bn\n",
      "85 add_8\n",
      "86 block10_sepconv1_act\n",
      "87 block10_sepconv1\n",
      "88 block10_sepconv1_bn\n",
      "89 block10_sepconv2_act\n",
      "90 block10_sepconv2\n",
      "91 block10_sepconv2_bn\n",
      "92 block10_sepconv3_act\n",
      "93 block10_sepconv3\n",
      "94 block10_sepconv3_bn\n",
      "95 add_9\n",
      "96 block11_sepconv1_act\n",
      "97 block11_sepconv1\n",
      "98 block11_sepconv1_bn\n",
      "99 block11_sepconv2_act\n",
      "100 block11_sepconv2\n",
      "101 block11_sepconv2_bn\n",
      "102 block11_sepconv3_act\n",
      "103 block11_sepconv3\n",
      "104 block11_sepconv3_bn\n",
      "105 add_10\n",
      "106 block12_sepconv1_act\n",
      "107 block12_sepconv1\n",
      "108 block12_sepconv1_bn\n",
      "109 block12_sepconv2_act\n",
      "110 block12_sepconv2\n",
      "111 block12_sepconv2_bn\n",
      "112 block12_sepconv3_act\n",
      "113 block12_sepconv3\n",
      "114 block12_sepconv3_bn\n",
      "115 add_11\n",
      "116 block13_sepconv1_act\n",
      "117 block13_sepconv1\n",
      "118 block13_sepconv1_bn\n",
      "119 block13_sepconv2_act\n",
      "120 block13_sepconv2\n",
      "121 block13_sepconv2_bn\n",
      "122 conv2d_18\n",
      "123 block13_pool\n",
      "124 batch_normalization_4\n",
      "125 add_12\n",
      "126 block14_sepconv1\n",
      "127 block14_sepconv1_bn\n",
      "128 block14_sepconv1_act\n",
      "129 block14_sepconv2\n",
      "130 block14_sepconv2_bn\n",
      "131 block14_sepconv2_act\n",
      "132 global_average_pooling2d_3\n",
      "133 dense_5\n",
      "134 dropout_3\n",
      "135 dense_6\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(xception_model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_2\n",
      "1 block1_conv1\n",
      "2 block1_conv2\n",
      "3 block1_pool\n",
      "4 block2_conv1\n",
      "5 block2_conv2\n",
      "6 block2_pool\n",
      "7 block3_conv1\n",
      "8 block3_conv2\n",
      "9 block3_conv3\n",
      "10 block3_pool\n",
      "11 block4_conv1\n",
      "12 block4_conv2\n",
      "13 block4_conv3\n",
      "14 block4_pool\n",
      "15 block5_conv1\n",
      "16 block5_conv2\n",
      "17 block5_conv3\n",
      "18 block5_pool\n",
      "19 global_average_pooling2d_4\n",
      "20 dense_7\n",
      "21 dropout_4\n",
      "22 dense_8\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(vgg_model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_3\n",
      "1 block1_conv1\n",
      "2 block1_conv2\n",
      "3 block1_pool\n",
      "4 block2_conv1\n",
      "5 block2_conv2\n",
      "6 block2_pool\n",
      "7 block3_conv1\n",
      "8 block3_conv2\n",
      "9 block3_conv3\n",
      "10 block3_conv4\n",
      "11 block3_pool\n",
      "12 block4_conv1\n",
      "13 block4_conv2\n",
      "14 block4_conv3\n",
      "15 block4_conv4\n",
      "16 block4_pool\n",
      "17 block5_conv1\n",
      "18 block5_conv2\n",
      "19 block5_conv3\n",
      "20 block5_conv4\n",
      "21 block5_pool\n",
      "22 global_average_pooling2d_5\n",
      "23 dense_9\n",
      "24 dropout_5\n",
      "25 dense_10\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(vgg19_model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_5\n",
      "1 conv2d_19\n",
      "2 batch_normalization_5\n",
      "3 activation_50\n",
      "4 conv2d_20\n",
      "5 batch_normalization_6\n",
      "6 activation_51\n",
      "7 conv2d_21\n",
      "8 batch_normalization_7\n",
      "9 activation_52\n",
      "10 max_pooling2d_6\n",
      "11 conv2d_22\n",
      "12 batch_normalization_8\n",
      "13 activation_53\n",
      "14 conv2d_23\n",
      "15 batch_normalization_9\n",
      "16 activation_54\n",
      "17 max_pooling2d_7\n",
      "18 conv2d_27\n",
      "19 batch_normalization_13\n",
      "20 activation_58\n",
      "21 conv2d_25\n",
      "22 conv2d_28\n",
      "23 batch_normalization_11\n",
      "24 batch_normalization_14\n",
      "25 activation_56\n",
      "26 activation_59\n",
      "27 average_pooling2d_1\n",
      "28 conv2d_24\n",
      "29 conv2d_26\n",
      "30 conv2d_29\n",
      "31 conv2d_30\n",
      "32 batch_normalization_10\n",
      "33 batch_normalization_12\n",
      "34 batch_normalization_15\n",
      "35 batch_normalization_16\n",
      "36 activation_55\n",
      "37 activation_57\n",
      "38 activation_60\n",
      "39 activation_61\n",
      "40 mixed0\n",
      "41 conv2d_34\n",
      "42 batch_normalization_20\n",
      "43 activation_65\n",
      "44 conv2d_32\n",
      "45 conv2d_35\n",
      "46 batch_normalization_18\n",
      "47 batch_normalization_21\n",
      "48 activation_63\n",
      "49 activation_66\n",
      "50 average_pooling2d_2\n",
      "51 conv2d_31\n",
      "52 conv2d_33\n",
      "53 conv2d_36\n",
      "54 conv2d_37\n",
      "55 batch_normalization_17\n",
      "56 batch_normalization_19\n",
      "57 batch_normalization_22\n",
      "58 batch_normalization_23\n",
      "59 activation_62\n",
      "60 activation_64\n",
      "61 activation_67\n",
      "62 activation_68\n",
      "63 mixed1\n",
      "64 conv2d_41\n",
      "65 batch_normalization_27\n",
      "66 activation_72\n",
      "67 conv2d_39\n",
      "68 conv2d_42\n",
      "69 batch_normalization_25\n",
      "70 batch_normalization_28\n",
      "71 activation_70\n",
      "72 activation_73\n",
      "73 average_pooling2d_3\n",
      "74 conv2d_38\n",
      "75 conv2d_40\n",
      "76 conv2d_43\n",
      "77 conv2d_44\n",
      "78 batch_normalization_24\n",
      "79 batch_normalization_26\n",
      "80 batch_normalization_29\n",
      "81 batch_normalization_30\n",
      "82 activation_69\n",
      "83 activation_71\n",
      "84 activation_74\n",
      "85 activation_75\n",
      "86 mixed2\n",
      "87 conv2d_46\n",
      "88 batch_normalization_32\n",
      "89 activation_77\n",
      "90 conv2d_47\n",
      "91 batch_normalization_33\n",
      "92 activation_78\n",
      "93 conv2d_45\n",
      "94 conv2d_48\n",
      "95 batch_normalization_31\n",
      "96 batch_normalization_34\n",
      "97 activation_76\n",
      "98 activation_79\n",
      "99 max_pooling2d_8\n",
      "100 mixed3\n",
      "101 conv2d_53\n",
      "102 batch_normalization_39\n",
      "103 activation_84\n",
      "104 conv2d_54\n",
      "105 batch_normalization_40\n",
      "106 activation_85\n",
      "107 conv2d_50\n",
      "108 conv2d_55\n",
      "109 batch_normalization_36\n",
      "110 batch_normalization_41\n",
      "111 activation_81\n",
      "112 activation_86\n",
      "113 conv2d_51\n",
      "114 conv2d_56\n",
      "115 batch_normalization_37\n",
      "116 batch_normalization_42\n",
      "117 activation_82\n",
      "118 activation_87\n",
      "119 average_pooling2d_4\n",
      "120 conv2d_49\n",
      "121 conv2d_52\n",
      "122 conv2d_57\n",
      "123 conv2d_58\n",
      "124 batch_normalization_35\n",
      "125 batch_normalization_38\n",
      "126 batch_normalization_43\n",
      "127 batch_normalization_44\n",
      "128 activation_80\n",
      "129 activation_83\n",
      "130 activation_88\n",
      "131 activation_89\n",
      "132 mixed4\n",
      "133 conv2d_63\n",
      "134 batch_normalization_49\n",
      "135 activation_94\n",
      "136 conv2d_64\n",
      "137 batch_normalization_50\n",
      "138 activation_95\n",
      "139 conv2d_60\n",
      "140 conv2d_65\n",
      "141 batch_normalization_46\n",
      "142 batch_normalization_51\n",
      "143 activation_91\n",
      "144 activation_96\n",
      "145 conv2d_61\n",
      "146 conv2d_66\n",
      "147 batch_normalization_47\n",
      "148 batch_normalization_52\n",
      "149 activation_92\n",
      "150 activation_97\n",
      "151 average_pooling2d_5\n",
      "152 conv2d_59\n",
      "153 conv2d_62\n",
      "154 conv2d_67\n",
      "155 conv2d_68\n",
      "156 batch_normalization_45\n",
      "157 batch_normalization_48\n",
      "158 batch_normalization_53\n",
      "159 batch_normalization_54\n",
      "160 activation_90\n",
      "161 activation_93\n",
      "162 activation_98\n",
      "163 activation_99\n",
      "164 mixed5\n",
      "165 conv2d_73\n",
      "166 batch_normalization_59\n",
      "167 activation_104\n",
      "168 conv2d_74\n",
      "169 batch_normalization_60\n",
      "170 activation_105\n",
      "171 conv2d_70\n",
      "172 conv2d_75\n",
      "173 batch_normalization_56\n",
      "174 batch_normalization_61\n",
      "175 activation_101\n",
      "176 activation_106\n",
      "177 conv2d_71\n",
      "178 conv2d_76\n",
      "179 batch_normalization_57\n",
      "180 batch_normalization_62\n",
      "181 activation_102\n",
      "182 activation_107\n",
      "183 average_pooling2d_6\n",
      "184 conv2d_69\n",
      "185 conv2d_72\n",
      "186 conv2d_77\n",
      "187 conv2d_78\n",
      "188 batch_normalization_55\n",
      "189 batch_normalization_58\n",
      "190 batch_normalization_63\n",
      "191 batch_normalization_64\n",
      "192 activation_100\n",
      "193 activation_103\n",
      "194 activation_108\n",
      "195 activation_109\n",
      "196 mixed6\n",
      "197 conv2d_83\n",
      "198 batch_normalization_69\n",
      "199 activation_114\n",
      "200 conv2d_84\n",
      "201 batch_normalization_70\n",
      "202 activation_115\n",
      "203 conv2d_80\n",
      "204 conv2d_85\n",
      "205 batch_normalization_66\n",
      "206 batch_normalization_71\n",
      "207 activation_111\n",
      "208 activation_116\n",
      "209 conv2d_81\n",
      "210 conv2d_86\n",
      "211 batch_normalization_67\n",
      "212 batch_normalization_72\n",
      "213 activation_112\n",
      "214 activation_117\n",
      "215 average_pooling2d_7\n",
      "216 conv2d_79\n",
      "217 conv2d_82\n",
      "218 conv2d_87\n",
      "219 conv2d_88\n",
      "220 batch_normalization_65\n",
      "221 batch_normalization_68\n",
      "222 batch_normalization_73\n",
      "223 batch_normalization_74\n",
      "224 activation_110\n",
      "225 activation_113\n",
      "226 activation_118\n",
      "227 activation_119\n",
      "228 mixed7\n",
      "229 conv2d_91\n",
      "230 batch_normalization_77\n",
      "231 activation_122\n",
      "232 conv2d_92\n",
      "233 batch_normalization_78\n",
      "234 activation_123\n",
      "235 conv2d_89\n",
      "236 conv2d_93\n",
      "237 batch_normalization_75\n",
      "238 batch_normalization_79\n",
      "239 activation_120\n",
      "240 activation_124\n",
      "241 conv2d_90\n",
      "242 conv2d_94\n",
      "243 batch_normalization_76\n",
      "244 batch_normalization_80\n",
      "245 activation_121\n",
      "246 activation_125\n",
      "247 max_pooling2d_9\n",
      "248 mixed8\n",
      "249 conv2d_99\n",
      "250 batch_normalization_85\n",
      "251 activation_130\n",
      "252 conv2d_96\n",
      "253 conv2d_100\n",
      "254 batch_normalization_82\n",
      "255 batch_normalization_86\n",
      "256 activation_127\n",
      "257 activation_131\n",
      "258 conv2d_97\n",
      "259 conv2d_98\n",
      "260 conv2d_101\n",
      "261 conv2d_102\n",
      "262 average_pooling2d_8\n",
      "263 conv2d_95\n",
      "264 batch_normalization_83\n",
      "265 batch_normalization_84\n",
      "266 batch_normalization_87\n",
      "267 batch_normalization_88\n",
      "268 conv2d_103\n",
      "269 batch_normalization_81\n",
      "270 activation_128\n",
      "271 activation_129\n",
      "272 activation_132\n",
      "273 activation_133\n",
      "274 batch_normalization_89\n",
      "275 activation_126\n",
      "276 mixed9_0\n",
      "277 concatenate_1\n",
      "278 activation_134\n",
      "279 mixed9\n",
      "280 conv2d_108\n",
      "281 batch_normalization_94\n",
      "282 activation_139\n",
      "283 conv2d_105\n",
      "284 conv2d_109\n",
      "285 batch_normalization_91\n",
      "286 batch_normalization_95\n",
      "287 activation_136\n",
      "288 activation_140\n",
      "289 conv2d_106\n",
      "290 conv2d_107\n",
      "291 conv2d_110\n",
      "292 conv2d_111\n",
      "293 average_pooling2d_9\n",
      "294 conv2d_104\n",
      "295 batch_normalization_92\n",
      "296 batch_normalization_93\n",
      "297 batch_normalization_96\n",
      "298 batch_normalization_97\n",
      "299 conv2d_112\n",
      "300 batch_normalization_90\n",
      "301 activation_137\n",
      "302 activation_138\n",
      "303 activation_141\n",
      "304 activation_142\n",
      "305 batch_normalization_98\n",
      "306 activation_135\n",
      "307 mixed9_1\n",
      "308 concatenate_2\n",
      "309 activation_143\n",
      "310 mixed10\n",
      "311 global_average_pooling2d_7\n",
      "312 dense_13\n",
      "313 dropout_7\n",
      "314 dense_14\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(incep_model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_6\n",
      "1 conv2d_113\n",
      "2 batch_normalization_99\n",
      "3 activation_144\n",
      "4 conv2d_114\n",
      "5 batch_normalization_100\n",
      "6 activation_145\n",
      "7 conv2d_115\n",
      "8 batch_normalization_101\n",
      "9 activation_146\n",
      "10 max_pooling2d_10\n",
      "11 conv2d_116\n",
      "12 batch_normalization_102\n",
      "13 activation_147\n",
      "14 conv2d_117\n",
      "15 batch_normalization_103\n",
      "16 activation_148\n",
      "17 max_pooling2d_11\n",
      "18 conv2d_121\n",
      "19 batch_normalization_107\n",
      "20 activation_152\n",
      "21 conv2d_119\n",
      "22 conv2d_122\n",
      "23 batch_normalization_105\n",
      "24 batch_normalization_108\n",
      "25 activation_150\n",
      "26 activation_153\n",
      "27 average_pooling2d_10\n",
      "28 conv2d_118\n",
      "29 conv2d_120\n",
      "30 conv2d_123\n",
      "31 conv2d_124\n",
      "32 batch_normalization_104\n",
      "33 batch_normalization_106\n",
      "34 batch_normalization_109\n",
      "35 batch_normalization_110\n",
      "36 activation_149\n",
      "37 activation_151\n",
      "38 activation_154\n",
      "39 activation_155\n",
      "40 mixed_5b\n",
      "41 conv2d_128\n",
      "42 batch_normalization_114\n",
      "43 activation_159\n",
      "44 conv2d_126\n",
      "45 conv2d_129\n",
      "46 batch_normalization_112\n",
      "47 batch_normalization_115\n",
      "48 activation_157\n",
      "49 activation_160\n",
      "50 conv2d_125\n",
      "51 conv2d_127\n",
      "52 conv2d_130\n",
      "53 batch_normalization_111\n",
      "54 batch_normalization_113\n",
      "55 batch_normalization_116\n",
      "56 activation_156\n",
      "57 activation_158\n",
      "58 activation_161\n",
      "59 block35_1_mixed\n",
      "60 block35_1_conv\n",
      "61 block35_1\n",
      "62 block35_1_ac\n",
      "63 conv2d_134\n",
      "64 batch_normalization_120\n",
      "65 activation_165\n",
      "66 conv2d_132\n",
      "67 conv2d_135\n",
      "68 batch_normalization_118\n",
      "69 batch_normalization_121\n",
      "70 activation_163\n",
      "71 activation_166\n",
      "72 conv2d_131\n",
      "73 conv2d_133\n",
      "74 conv2d_136\n",
      "75 batch_normalization_117\n",
      "76 batch_normalization_119\n",
      "77 batch_normalization_122\n",
      "78 activation_162\n",
      "79 activation_164\n",
      "80 activation_167\n",
      "81 block35_2_mixed\n",
      "82 block35_2_conv\n",
      "83 block35_2\n",
      "84 block35_2_ac\n",
      "85 conv2d_140\n",
      "86 batch_normalization_126\n",
      "87 activation_171\n",
      "88 conv2d_138\n",
      "89 conv2d_141\n",
      "90 batch_normalization_124\n",
      "91 batch_normalization_127\n",
      "92 activation_169\n",
      "93 activation_172\n",
      "94 conv2d_137\n",
      "95 conv2d_139\n",
      "96 conv2d_142\n",
      "97 batch_normalization_123\n",
      "98 batch_normalization_125\n",
      "99 batch_normalization_128\n",
      "100 activation_168\n",
      "101 activation_170\n",
      "102 activation_173\n",
      "103 block35_3_mixed\n",
      "104 block35_3_conv\n",
      "105 block35_3\n",
      "106 block35_3_ac\n",
      "107 conv2d_146\n",
      "108 batch_normalization_132\n",
      "109 activation_177\n",
      "110 conv2d_144\n",
      "111 conv2d_147\n",
      "112 batch_normalization_130\n",
      "113 batch_normalization_133\n",
      "114 activation_175\n",
      "115 activation_178\n",
      "116 conv2d_143\n",
      "117 conv2d_145\n",
      "118 conv2d_148\n",
      "119 batch_normalization_129\n",
      "120 batch_normalization_131\n",
      "121 batch_normalization_134\n",
      "122 activation_174\n",
      "123 activation_176\n",
      "124 activation_179\n",
      "125 block35_4_mixed\n",
      "126 block35_4_conv\n",
      "127 block35_4\n",
      "128 block35_4_ac\n",
      "129 conv2d_152\n",
      "130 batch_normalization_138\n",
      "131 activation_183\n",
      "132 conv2d_150\n",
      "133 conv2d_153\n",
      "134 batch_normalization_136\n",
      "135 batch_normalization_139\n",
      "136 activation_181\n",
      "137 activation_184\n",
      "138 conv2d_149\n",
      "139 conv2d_151\n",
      "140 conv2d_154\n",
      "141 batch_normalization_135\n",
      "142 batch_normalization_137\n",
      "143 batch_normalization_140\n",
      "144 activation_180\n",
      "145 activation_182\n",
      "146 activation_185\n",
      "147 block35_5_mixed\n",
      "148 block35_5_conv\n",
      "149 block35_5\n",
      "150 block35_5_ac\n",
      "151 conv2d_158\n",
      "152 batch_normalization_144\n",
      "153 activation_189\n",
      "154 conv2d_156\n",
      "155 conv2d_159\n",
      "156 batch_normalization_142\n",
      "157 batch_normalization_145\n",
      "158 activation_187\n",
      "159 activation_190\n",
      "160 conv2d_155\n",
      "161 conv2d_157\n",
      "162 conv2d_160\n",
      "163 batch_normalization_141\n",
      "164 batch_normalization_143\n",
      "165 batch_normalization_146\n",
      "166 activation_186\n",
      "167 activation_188\n",
      "168 activation_191\n",
      "169 block35_6_mixed\n",
      "170 block35_6_conv\n",
      "171 block35_6\n",
      "172 block35_6_ac\n",
      "173 conv2d_164\n",
      "174 batch_normalization_150\n",
      "175 activation_195\n",
      "176 conv2d_162\n",
      "177 conv2d_165\n",
      "178 batch_normalization_148\n",
      "179 batch_normalization_151\n",
      "180 activation_193\n",
      "181 activation_196\n",
      "182 conv2d_161\n",
      "183 conv2d_163\n",
      "184 conv2d_166\n",
      "185 batch_normalization_147\n",
      "186 batch_normalization_149\n",
      "187 batch_normalization_152\n",
      "188 activation_192\n",
      "189 activation_194\n",
      "190 activation_197\n",
      "191 block35_7_mixed\n",
      "192 block35_7_conv\n",
      "193 block35_7\n",
      "194 block35_7_ac\n",
      "195 conv2d_170\n",
      "196 batch_normalization_156\n",
      "197 activation_201\n",
      "198 conv2d_168\n",
      "199 conv2d_171\n",
      "200 batch_normalization_154\n",
      "201 batch_normalization_157\n",
      "202 activation_199\n",
      "203 activation_202\n",
      "204 conv2d_167\n",
      "205 conv2d_169\n",
      "206 conv2d_172\n",
      "207 batch_normalization_153\n",
      "208 batch_normalization_155\n",
      "209 batch_normalization_158\n",
      "210 activation_198\n",
      "211 activation_200\n",
      "212 activation_203\n",
      "213 block35_8_mixed\n",
      "214 block35_8_conv\n",
      "215 block35_8\n",
      "216 block35_8_ac\n",
      "217 conv2d_176\n",
      "218 batch_normalization_162\n",
      "219 activation_207\n",
      "220 conv2d_174\n",
      "221 conv2d_177\n",
      "222 batch_normalization_160\n",
      "223 batch_normalization_163\n",
      "224 activation_205\n",
      "225 activation_208\n",
      "226 conv2d_173\n",
      "227 conv2d_175\n",
      "228 conv2d_178\n",
      "229 batch_normalization_159\n",
      "230 batch_normalization_161\n",
      "231 batch_normalization_164\n",
      "232 activation_204\n",
      "233 activation_206\n",
      "234 activation_209\n",
      "235 block35_9_mixed\n",
      "236 block35_9_conv\n",
      "237 block35_9\n",
      "238 block35_9_ac\n",
      "239 conv2d_182\n",
      "240 batch_normalization_168\n",
      "241 activation_213\n",
      "242 conv2d_180\n",
      "243 conv2d_183\n",
      "244 batch_normalization_166\n",
      "245 batch_normalization_169\n",
      "246 activation_211\n",
      "247 activation_214\n",
      "248 conv2d_179\n",
      "249 conv2d_181\n",
      "250 conv2d_184\n",
      "251 batch_normalization_165\n",
      "252 batch_normalization_167\n",
      "253 batch_normalization_170\n",
      "254 activation_210\n",
      "255 activation_212\n",
      "256 activation_215\n",
      "257 block35_10_mixed\n",
      "258 block35_10_conv\n",
      "259 block35_10\n",
      "260 block35_10_ac\n",
      "261 conv2d_186\n",
      "262 batch_normalization_172\n",
      "263 activation_217\n",
      "264 conv2d_187\n",
      "265 batch_normalization_173\n",
      "266 activation_218\n",
      "267 conv2d_185\n",
      "268 conv2d_188\n",
      "269 batch_normalization_171\n",
      "270 batch_normalization_174\n",
      "271 activation_216\n",
      "272 activation_219\n",
      "273 max_pooling2d_12\n",
      "274 mixed_6a\n",
      "275 conv2d_190\n",
      "276 batch_normalization_176\n",
      "277 activation_221\n",
      "278 conv2d_191\n",
      "279 batch_normalization_177\n",
      "280 activation_222\n",
      "281 conv2d_189\n",
      "282 conv2d_192\n",
      "283 batch_normalization_175\n",
      "284 batch_normalization_178\n",
      "285 activation_220\n",
      "286 activation_223\n",
      "287 block17_1_mixed\n",
      "288 block17_1_conv\n",
      "289 block17_1\n",
      "290 block17_1_ac\n",
      "291 conv2d_194\n",
      "292 batch_normalization_180\n",
      "293 activation_225\n",
      "294 conv2d_195\n",
      "295 batch_normalization_181\n",
      "296 activation_226\n",
      "297 conv2d_193\n",
      "298 conv2d_196\n",
      "299 batch_normalization_179\n",
      "300 batch_normalization_182\n",
      "301 activation_224\n",
      "302 activation_227\n",
      "303 block17_2_mixed\n",
      "304 block17_2_conv\n",
      "305 block17_2\n",
      "306 block17_2_ac\n",
      "307 conv2d_198\n",
      "308 batch_normalization_184\n",
      "309 activation_229\n",
      "310 conv2d_199\n",
      "311 batch_normalization_185\n",
      "312 activation_230\n",
      "313 conv2d_197\n",
      "314 conv2d_200\n",
      "315 batch_normalization_183\n",
      "316 batch_normalization_186\n",
      "317 activation_228\n",
      "318 activation_231\n",
      "319 block17_3_mixed\n",
      "320 block17_3_conv\n",
      "321 block17_3\n",
      "322 block17_3_ac\n",
      "323 conv2d_202\n",
      "324 batch_normalization_188\n",
      "325 activation_233\n",
      "326 conv2d_203\n",
      "327 batch_normalization_189\n",
      "328 activation_234\n",
      "329 conv2d_201\n",
      "330 conv2d_204\n",
      "331 batch_normalization_187\n",
      "332 batch_normalization_190\n",
      "333 activation_232\n",
      "334 activation_235\n",
      "335 block17_4_mixed\n",
      "336 block17_4_conv\n",
      "337 block17_4\n",
      "338 block17_4_ac\n",
      "339 conv2d_206\n",
      "340 batch_normalization_192\n",
      "341 activation_237\n",
      "342 conv2d_207\n",
      "343 batch_normalization_193\n",
      "344 activation_238\n",
      "345 conv2d_205\n",
      "346 conv2d_208\n",
      "347 batch_normalization_191\n",
      "348 batch_normalization_194\n",
      "349 activation_236\n",
      "350 activation_239\n",
      "351 block17_5_mixed\n",
      "352 block17_5_conv\n",
      "353 block17_5\n",
      "354 block17_5_ac\n",
      "355 conv2d_210\n",
      "356 batch_normalization_196\n",
      "357 activation_241\n",
      "358 conv2d_211\n",
      "359 batch_normalization_197\n",
      "360 activation_242\n",
      "361 conv2d_209\n",
      "362 conv2d_212\n",
      "363 batch_normalization_195\n",
      "364 batch_normalization_198\n",
      "365 activation_240\n",
      "366 activation_243\n",
      "367 block17_6_mixed\n",
      "368 block17_6_conv\n",
      "369 block17_6\n",
      "370 block17_6_ac\n",
      "371 conv2d_214\n",
      "372 batch_normalization_200\n",
      "373 activation_245\n",
      "374 conv2d_215\n",
      "375 batch_normalization_201\n",
      "376 activation_246\n",
      "377 conv2d_213\n",
      "378 conv2d_216\n",
      "379 batch_normalization_199\n",
      "380 batch_normalization_202\n",
      "381 activation_244\n",
      "382 activation_247\n",
      "383 block17_7_mixed\n",
      "384 block17_7_conv\n",
      "385 block17_7\n",
      "386 block17_7_ac\n",
      "387 conv2d_218\n",
      "388 batch_normalization_204\n",
      "389 activation_249\n",
      "390 conv2d_219\n",
      "391 batch_normalization_205\n",
      "392 activation_250\n",
      "393 conv2d_217\n",
      "394 conv2d_220\n",
      "395 batch_normalization_203\n",
      "396 batch_normalization_206\n",
      "397 activation_248\n",
      "398 activation_251\n",
      "399 block17_8_mixed\n",
      "400 block17_8_conv\n",
      "401 block17_8\n",
      "402 block17_8_ac\n",
      "403 conv2d_222\n",
      "404 batch_normalization_208\n",
      "405 activation_253\n",
      "406 conv2d_223\n",
      "407 batch_normalization_209\n",
      "408 activation_254\n",
      "409 conv2d_221\n",
      "410 conv2d_224\n",
      "411 batch_normalization_207\n",
      "412 batch_normalization_210\n",
      "413 activation_252\n",
      "414 activation_255\n",
      "415 block17_9_mixed\n",
      "416 block17_9_conv\n",
      "417 block17_9\n",
      "418 block17_9_ac\n",
      "419 conv2d_226\n",
      "420 batch_normalization_212\n",
      "421 activation_257\n",
      "422 conv2d_227\n",
      "423 batch_normalization_213\n",
      "424 activation_258\n",
      "425 conv2d_225\n",
      "426 conv2d_228\n",
      "427 batch_normalization_211\n",
      "428 batch_normalization_214\n",
      "429 activation_256\n",
      "430 activation_259\n",
      "431 block17_10_mixed\n",
      "432 block17_10_conv\n",
      "433 block17_10\n",
      "434 block17_10_ac\n",
      "435 conv2d_230\n",
      "436 batch_normalization_216\n",
      "437 activation_261\n",
      "438 conv2d_231\n",
      "439 batch_normalization_217\n",
      "440 activation_262\n",
      "441 conv2d_229\n",
      "442 conv2d_232\n",
      "443 batch_normalization_215\n",
      "444 batch_normalization_218\n",
      "445 activation_260\n",
      "446 activation_263\n",
      "447 block17_11_mixed\n",
      "448 block17_11_conv\n",
      "449 block17_11\n",
      "450 block17_11_ac\n",
      "451 conv2d_234\n",
      "452 batch_normalization_220\n",
      "453 activation_265\n",
      "454 conv2d_235\n",
      "455 batch_normalization_221\n",
      "456 activation_266\n",
      "457 conv2d_233\n",
      "458 conv2d_236\n",
      "459 batch_normalization_219\n",
      "460 batch_normalization_222\n",
      "461 activation_264\n",
      "462 activation_267\n",
      "463 block17_12_mixed\n",
      "464 block17_12_conv\n",
      "465 block17_12\n",
      "466 block17_12_ac\n",
      "467 conv2d_238\n",
      "468 batch_normalization_224\n",
      "469 activation_269\n",
      "470 conv2d_239\n",
      "471 batch_normalization_225\n",
      "472 activation_270\n",
      "473 conv2d_237\n",
      "474 conv2d_240\n",
      "475 batch_normalization_223\n",
      "476 batch_normalization_226\n",
      "477 activation_268\n",
      "478 activation_271\n",
      "479 block17_13_mixed\n",
      "480 block17_13_conv\n",
      "481 block17_13\n",
      "482 block17_13_ac\n",
      "483 conv2d_242\n",
      "484 batch_normalization_228\n",
      "485 activation_273\n",
      "486 conv2d_243\n",
      "487 batch_normalization_229\n",
      "488 activation_274\n",
      "489 conv2d_241\n",
      "490 conv2d_244\n",
      "491 batch_normalization_227\n",
      "492 batch_normalization_230\n",
      "493 activation_272\n",
      "494 activation_275\n",
      "495 block17_14_mixed\n",
      "496 block17_14_conv\n",
      "497 block17_14\n",
      "498 block17_14_ac\n",
      "499 conv2d_246\n",
      "500 batch_normalization_232\n",
      "501 activation_277\n",
      "502 conv2d_247\n",
      "503 batch_normalization_233\n",
      "504 activation_278\n",
      "505 conv2d_245\n",
      "506 conv2d_248\n",
      "507 batch_normalization_231\n",
      "508 batch_normalization_234\n",
      "509 activation_276\n",
      "510 activation_279\n",
      "511 block17_15_mixed\n",
      "512 block17_15_conv\n",
      "513 block17_15\n",
      "514 block17_15_ac\n",
      "515 conv2d_250\n",
      "516 batch_normalization_236\n",
      "517 activation_281\n",
      "518 conv2d_251\n",
      "519 batch_normalization_237\n",
      "520 activation_282\n",
      "521 conv2d_249\n",
      "522 conv2d_252\n",
      "523 batch_normalization_235\n",
      "524 batch_normalization_238\n",
      "525 activation_280\n",
      "526 activation_283\n",
      "527 block17_16_mixed\n",
      "528 block17_16_conv\n",
      "529 block17_16\n",
      "530 block17_16_ac\n",
      "531 conv2d_254\n",
      "532 batch_normalization_240\n",
      "533 activation_285\n",
      "534 conv2d_255\n",
      "535 batch_normalization_241\n",
      "536 activation_286\n",
      "537 conv2d_253\n",
      "538 conv2d_256\n",
      "539 batch_normalization_239\n",
      "540 batch_normalization_242\n",
      "541 activation_284\n",
      "542 activation_287\n",
      "543 block17_17_mixed\n",
      "544 block17_17_conv\n",
      "545 block17_17\n",
      "546 block17_17_ac\n",
      "547 conv2d_258\n",
      "548 batch_normalization_244\n",
      "549 activation_289\n",
      "550 conv2d_259\n",
      "551 batch_normalization_245\n",
      "552 activation_290\n",
      "553 conv2d_257\n",
      "554 conv2d_260\n",
      "555 batch_normalization_243\n",
      "556 batch_normalization_246\n",
      "557 activation_288\n",
      "558 activation_291\n",
      "559 block17_18_mixed\n",
      "560 block17_18_conv\n",
      "561 block17_18\n",
      "562 block17_18_ac\n",
      "563 conv2d_262\n",
      "564 batch_normalization_248\n",
      "565 activation_293\n",
      "566 conv2d_263\n",
      "567 batch_normalization_249\n",
      "568 activation_294\n",
      "569 conv2d_261\n",
      "570 conv2d_264\n",
      "571 batch_normalization_247\n",
      "572 batch_normalization_250\n",
      "573 activation_292\n",
      "574 activation_295\n",
      "575 block17_19_mixed\n",
      "576 block17_19_conv\n",
      "577 block17_19\n",
      "578 block17_19_ac\n",
      "579 conv2d_266\n",
      "580 batch_normalization_252\n",
      "581 activation_297\n",
      "582 conv2d_267\n",
      "583 batch_normalization_253\n",
      "584 activation_298\n",
      "585 conv2d_265\n",
      "586 conv2d_268\n",
      "587 batch_normalization_251\n",
      "588 batch_normalization_254\n",
      "589 activation_296\n",
      "590 activation_299\n",
      "591 block17_20_mixed\n",
      "592 block17_20_conv\n",
      "593 block17_20\n",
      "594 block17_20_ac\n",
      "595 conv2d_273\n",
      "596 batch_normalization_259\n",
      "597 activation_304\n",
      "598 conv2d_269\n",
      "599 conv2d_271\n",
      "600 conv2d_274\n",
      "601 batch_normalization_255\n",
      "602 batch_normalization_257\n",
      "603 batch_normalization_260\n",
      "604 activation_300\n",
      "605 activation_302\n",
      "606 activation_305\n",
      "607 conv2d_270\n",
      "608 conv2d_272\n",
      "609 conv2d_275\n",
      "610 batch_normalization_256\n",
      "611 batch_normalization_258\n",
      "612 batch_normalization_261\n",
      "613 activation_301\n",
      "614 activation_303\n",
      "615 activation_306\n",
      "616 max_pooling2d_13\n",
      "617 mixed_7a\n",
      "618 conv2d_277\n",
      "619 batch_normalization_263\n",
      "620 activation_308\n",
      "621 conv2d_278\n",
      "622 batch_normalization_264\n",
      "623 activation_309\n",
      "624 conv2d_276\n",
      "625 conv2d_279\n",
      "626 batch_normalization_262\n",
      "627 batch_normalization_265\n",
      "628 activation_307\n",
      "629 activation_310\n",
      "630 block8_1_mixed\n",
      "631 block8_1_conv\n",
      "632 block8_1\n",
      "633 block8_1_ac\n",
      "634 conv2d_281\n",
      "635 batch_normalization_267\n",
      "636 activation_312\n",
      "637 conv2d_282\n",
      "638 batch_normalization_268\n",
      "639 activation_313\n",
      "640 conv2d_280\n",
      "641 conv2d_283\n",
      "642 batch_normalization_266\n",
      "643 batch_normalization_269\n",
      "644 activation_311\n",
      "645 activation_314\n",
      "646 block8_2_mixed\n",
      "647 block8_2_conv\n",
      "648 block8_2\n",
      "649 block8_2_ac\n",
      "650 conv2d_285\n",
      "651 batch_normalization_271\n",
      "652 activation_316\n",
      "653 conv2d_286\n",
      "654 batch_normalization_272\n",
      "655 activation_317\n",
      "656 conv2d_284\n",
      "657 conv2d_287\n",
      "658 batch_normalization_270\n",
      "659 batch_normalization_273\n",
      "660 activation_315\n",
      "661 activation_318\n",
      "662 block8_3_mixed\n",
      "663 block8_3_conv\n",
      "664 block8_3\n",
      "665 block8_3_ac\n",
      "666 conv2d_289\n",
      "667 batch_normalization_275\n",
      "668 activation_320\n",
      "669 conv2d_290\n",
      "670 batch_normalization_276\n",
      "671 activation_321\n",
      "672 conv2d_288\n",
      "673 conv2d_291\n",
      "674 batch_normalization_274\n",
      "675 batch_normalization_277\n",
      "676 activation_319\n",
      "677 activation_322\n",
      "678 block8_4_mixed\n",
      "679 block8_4_conv\n",
      "680 block8_4\n",
      "681 block8_4_ac\n",
      "682 conv2d_293\n",
      "683 batch_normalization_279\n",
      "684 activation_324\n",
      "685 conv2d_294\n",
      "686 batch_normalization_280\n",
      "687 activation_325\n",
      "688 conv2d_292\n",
      "689 conv2d_295\n",
      "690 batch_normalization_278\n",
      "691 batch_normalization_281\n",
      "692 activation_323\n",
      "693 activation_326\n",
      "694 block8_5_mixed\n",
      "695 block8_5_conv\n",
      "696 block8_5\n",
      "697 block8_5_ac\n",
      "698 conv2d_297\n",
      "699 batch_normalization_283\n",
      "700 activation_328\n",
      "701 conv2d_298\n",
      "702 batch_normalization_284\n",
      "703 activation_329\n",
      "704 conv2d_296\n",
      "705 conv2d_299\n",
      "706 batch_normalization_282\n",
      "707 batch_normalization_285\n",
      "708 activation_327\n",
      "709 activation_330\n",
      "710 block8_6_mixed\n",
      "711 block8_6_conv\n",
      "712 block8_6\n",
      "713 block8_6_ac\n",
      "714 conv2d_301\n",
      "715 batch_normalization_287\n",
      "716 activation_332\n",
      "717 conv2d_302\n",
      "718 batch_normalization_288\n",
      "719 activation_333\n",
      "720 conv2d_300\n",
      "721 conv2d_303\n",
      "722 batch_normalization_286\n",
      "723 batch_normalization_289\n",
      "724 activation_331\n",
      "725 activation_334\n",
      "726 block8_7_mixed\n",
      "727 block8_7_conv\n",
      "728 block8_7\n",
      "729 block8_7_ac\n",
      "730 conv2d_305\n",
      "731 batch_normalization_291\n",
      "732 activation_336\n",
      "733 conv2d_306\n",
      "734 batch_normalization_292\n",
      "735 activation_337\n",
      "736 conv2d_304\n",
      "737 conv2d_307\n",
      "738 batch_normalization_290\n",
      "739 batch_normalization_293\n",
      "740 activation_335\n",
      "741 activation_338\n",
      "742 block8_8_mixed\n",
      "743 block8_8_conv\n",
      "744 block8_8\n",
      "745 block8_8_ac\n",
      "746 conv2d_309\n",
      "747 batch_normalization_295\n",
      "748 activation_340\n",
      "749 conv2d_310\n",
      "750 batch_normalization_296\n",
      "751 activation_341\n",
      "752 conv2d_308\n",
      "753 conv2d_311\n",
      "754 batch_normalization_294\n",
      "755 batch_normalization_297\n",
      "756 activation_339\n",
      "757 activation_342\n",
      "758 block8_9_mixed\n",
      "759 block8_9_conv\n",
      "760 block8_9\n",
      "761 block8_9_ac\n",
      "762 conv2d_313\n",
      "763 batch_normalization_299\n",
      "764 activation_344\n",
      "765 conv2d_314\n",
      "766 batch_normalization_300\n",
      "767 activation_345\n",
      "768 conv2d_312\n",
      "769 conv2d_315\n",
      "770 batch_normalization_298\n",
      "771 batch_normalization_301\n",
      "772 activation_343\n",
      "773 activation_346\n",
      "774 block8_10_mixed\n",
      "775 block8_10_conv\n",
      "776 block8_10\n",
      "777 conv_7b\n",
      "778 conv_7b_bn\n",
      "779 conv_7b_ac\n",
      "780 global_average_pooling2d_8\n",
      "781 dense_15\n",
      "782 dropout_8\n",
      "783 dense_16\n"
     ]
    }
   ],
   "source": [
    "for i,layer in enumerate(inres_model.layers):\n",
    "    print(i,layer.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the ensemble model using the last \"dense layer\" of each base CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model1=Model(inputs=[xception_model.layers[0].get_input_at(0)],outputs=xception_model.get_layer('dense_6').output,name='xception')\n",
    "model2=Model(inputs=[vgg_model.layers[0].get_input_at(0)],outputs=vgg_model.get_layer('dense_8').output,name='vgg')\n",
    "model3=Model(inputs=[vgg19_model.layers[0].get_input_at(0)],outputs=vgg19_model.get_layer('dense_10').output,name='vgg19')\n",
    "model4=Model(inputs=[incep_model.layers[0].get_input_at(0)],outputs=incep_model.get_layer('dense_14').output,name='incep')\n",
    "model5=Model(inputs=[inres_model.layers[0].get_input_at(0)],outputs=inres_model.get_layer('dense_16').output,name='inres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot the figures\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        if loss_type == 'epoch':\n",
    "            # acc\n",
    "            plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "            # loss\n",
    "            plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_history= LossHistory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23382 images belonging to 5 classes.\n",
      "Found 5845 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "#generate training and test images\n",
    "TARGET_SIZE=(224,224)\n",
    "INPUT_SIZE=(224,224,3)\n",
    "BATCHSIZE=128\t#could try 128 or 32\n",
    "\n",
    "#Normalization\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        './train_224/',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        './test_224/',\n",
    "        target_size=TARGET_SIZE,\n",
    "        batch_size=BATCHSIZE,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lr_decay(epoch):\n",
    "    lrs = [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.0001,0.00001,0.000001,\n",
    "           0.000001,0.000001,0.000001,0.000001,0.0000001,0.0000001,0.0000001,0.0000001,0.0000001,0.0000001\n",
    "          ]\n",
    "    return lrs[epoch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auto_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "my_lr = LearningRateScheduler(lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ensemble(num_class,epochs,savepath='./ensemble.h5'):\n",
    "    img=Input(shape=(224,224,3),name='img')\n",
    "    feature1=model1(img)\n",
    "    feature2=model2(img)\n",
    "    feature3=model3(img)\n",
    "    x=concatenate([feature1,feature2,feature3])\n",
    "    x=Dropout(0.5)(x)\n",
    "    x=Dense(64,activation='relu')(x)\n",
    "    x=Dropout(0.25)(x)\n",
    "    output=Dense(num_class,activation='softmax',name='output')(x)\n",
    "    model=Model(inputs=img,outputs=output)\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=['accuracy'])\n",
    "    #train model\n",
    "    earlyStopping=kcallbacks.EarlyStopping(monitor='val_acc',patience=2, verbose=1, mode='auto')\n",
    "    saveBestModel = kcallbacks.ModelCheckpoint(filepath=savepath, monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "    hist=model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=epochs,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=len(validation_generator),\n",
    "        callbacks=[earlyStopping,saveBestModel,ensemble_history,auto_lr],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.5297 - acc: 0.8809Epoch 00001: val_acc improved from -inf to 1.00000, saving model to ./ensemble.h5\n",
      "183/183 [==============================] - 204s 1s/step - loss: 0.5278 - acc: 0.8814 - val_loss: 0.0590 - val_acc: 1.0000\n",
      "Epoch 2/20\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.1287 - acc: 0.9770Epoch 00002: val_acc did not improve\n",
      "183/183 [==============================] - 184s 1s/step - loss: 0.1286 - acc: 0.9770 - val_loss: 0.0103 - val_acc: 1.0000\n",
      "Epoch 3/20\n",
      "182/183 [============================>.] - ETA: 0s - loss: 0.0975 - acc: 0.9778Epoch 00003: val_acc did not improve\n",
      "183/183 [==============================] - 184s 1s/step - loss: 0.0973 - acc: 0.9778 - val_loss: 0.0033 - val_acc: 1.0000\n",
      "Epoch 00003: early stopping\n"
     ]
    }
   ],
   "source": [
    "ensemble_model=ensemble(num_class=5,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ensemble_model=load_model('./ensemble.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ./test_224/0\\100015.png\n"
     ]
    }
   ],
   "source": [
    "#read images from validation folder\n",
    "rootdir = './test_224/'\n",
    "test_laels = []\n",
    "test_images=[]\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        if not (file.endswith(\".jpeg\"))|(file.endswith(\".jpg\"))|(file.endswith(\".png\")):\n",
    "            continue\n",
    "        test_laels.append(subdir.split('/')[-1])\n",
    "        test_images.append(os.path.join(subdir, file))\n",
    "        \n",
    "print(test_laels[0],test_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The testing time is :54.200418 seconds\n"
     ]
    }
   ],
   "source": [
    "#test the averaging model on the validation set\n",
    "import time\n",
    "predict=[]\n",
    "length=len(test_images)\n",
    "t1 = time.time()\n",
    "for i in range((length//127)+1):\n",
    "    inputimg=test_images[127*i:127*(i+1)]\n",
    "    test_batch=[]\n",
    "    for path in inputimg:\n",
    "        thisimg=np.array(Image.open(path))/255\n",
    "        test_batch.append(thisimg)\n",
    "    #print(i, np.array(test_batch).shape)\n",
    "    ensemble_model_batch=ensemble_model.predict(np.array(test_batch))\n",
    "    predict_batch=list(np.argmax(ensemble_model_batch,axis=1))\n",
    "    predict_batch=[label[con] for con in predict_batch]\n",
    "    predict.append(predict_batch)\n",
    "\n",
    "predict=sum(predict,[])\n",
    "\n",
    "t2 = time.time()\n",
    "print('The testing time is :%f seconds' % (t2-t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenation accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "acc=accuracy_score(test_laels,predict)\n",
    "print('Concatenation accuracy:%s'%acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5052    0    0    0    0]\n",
      " [   0  225    0    0    0]\n",
      " [   0    0  200    0    0]\n",
      " [   0    0    0  197    0]\n",
      " [   0    0    0    0  171]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      5052\n",
      "           1       1.00      1.00      1.00       225\n",
      "           2       1.00      1.00      1.00       200\n",
      "           3       1.00      1.00      1.00       197\n",
      "           4       1.00      1.00      1.00       171\n",
      "\n",
      "    accuracy                           1.00      5845\n",
      "   macro avg       1.00      1.00      1.00      5845\n",
      "weighted avg       1.00      1.00      1.00      5845\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(test_laels, predict))\n",
    "target_names = ['0', '1','2','3','4']\n",
    "print(classification_report(test_laels, predict, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "tf36cnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
